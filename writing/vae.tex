\chapter{Variational Autoencoders}\label{chap:vae}

Differently from ordinary autoencoding neural networks that we already mentioned to be discriminative models, variational autoencoding neural networks on the other hand are so called generative models, see \cite[Chapter~5]{cinelli2021variational}. This means that instead of trying to estimate the conditional distribution of $y|x$, where $y$ is a predicted label to an observation $x$, variational autoencoders attempt to capture the entire probability distribution of $Y$. This is very interesting for multiple reasons, since this means that we can simulate and anticipate the evolution of the model output. Hence, we could generate new data based on the captured probability distribution. This will be our ultimate goal in this chapter, considering applications at last.

\section{Probabilistic foundations}

Before diving into the depths of variational autoencoders, we want to begin by laying the essential probabilistic foundations. As already mentioned, in contrast to ordinary autoencoding neural networks which we discussed in Chapter~\ref{chap:ae}
variational autoencoding neural networks attempt to capture the entire probability distribution of $Y$. So the emerging question is how can we infer the probability distribution of $Y$, if we only have the observations $x$ given? We assume that the single data points $x_i$ are not independent - what is no grave assumption, since the observations are assumed to have a reason to be shaped the way they are (e.g. having the same underlying probability distribution). This reason, since it is unknown or \textit{latent} to us, we will try to consider in more detail. In order to model this hidden cause, we introduce another random variable $Z$, a so called \textit{latent variable} and thus by marginalizing over $z$ obtain the joint distribution $\prob_{X, Z}$

\begin{align}\label{eq:evidence}
\prob_X(x) = \int_y \prob_{X,Z}(x,z) dz = \int \prob_{X|Z}(x|z)\prob_{Z}(z)dz = \E \left( \prob_{X|Z}(x|z) \right),
\end{align}


as we considered in Section \ref{sec:preliminary_prob}
This probability distribution we now want to introduce formally, since it will be of crucial importance in this chapter. However, we first need to introduce the latent space and the observation space.

\begin{definition}\label{def:random_variables}
Let $(\O, \A, \prob)$ be a probability space. Furthermore, let $X:\O\to\R$, $Z:\O\to\R$ be random variables, where the marginal probability distribution $\prob_X$ is known and the marginal probability distribution $\prob_Z$ is unknown.\\
We will call $X$ the \textbf{observation random variable} and $Z$ the \textbf{latent random variable} in the following.
\end{definition}

Considering the joint and marginal distributions, which are induced from the two distributions introduced in Definition \ref{def:random_variables}, we can define fundamental quantities as e.g. model evidence and model prior for the Bayesian Learning setting. These we want to introduce in the following proposition.

\begin{proposition}
Let $X$, $Y$ be observation and latent random variable, respectively. In the following, we will refer to the marginal probability $\prob_X$ of $X$ as \textbf{model evidence} and to the marginal probability $\prob_Z$ of $Z$ as \textbf{model prior}.
\end{proposition}

Since our ultimate goal is to find a probability distribution that describes our observations as good as possible, we need to somehow define an update rule, how we want to adapt our current model of the latent space with respect to newly observed data. In order to do this, we need to introduce two more quantities. The first one is the so called likelihood. Conceptionally, it is like a probability density function, where we fix the observation and let the parameter be a variable. The second quantity is called the posterior - as the name already suggests, this describes our understanding of the latent space after considering newly observed data.

\begin{definition}\label{def:likelihood}
Let $(\O,\A, \prob)$ be a probability space, $(\mathcal{X}, \A)$ be a measurable space on which we define a random variable $X$ and let $\T$ be a parameter space. Furthermore, let $X$ have a probability distribution with parameters $\t\in\T$. Hence, we describe the probability density function of $X$ as $p(x;\t)$.\\
Then we define the \textbf{likelihood function} as
\begin{align*}
L(\t | x) = p(x; \t),
\end{align*}
where $x\in\O$ is fixed and $\t\in\T$ is variable.
\end{definition}

\begin{definition}\label{def:posterior}
Let $(\O,\A, \prob)$ be a probability space, $(\mathcal{X}, \A)$ be a measurable space on which we define a random variable $X$ and let $\T$ be a parameter space. Furthermore, let $X$ have a probability distribution with parameters $\t\in\T$. Hence, we describe the probability density function of $X$ as $p(x;\t)$.\\
Then we define the \textbf{model posterior} as $p(\t | X)$.
\end{definition}

\begin{remark}
Since it is common to make no distinction between random variables and parameters in a Bayesian approach, we can consider the model parameters $\t\in\T$ as part of the latent space. Hence, we can describe the likelihood function as $p(x|Z)$.\\
Moreover, with the same argument we can consider the model posterior as $p(z|X)$.
\end{remark}


\textcolor{red}{FROM HERE ON THE TEXT IS NOT REWORKED YET}

Firstly, we want to define the specific structure of variational autoencoding neural networks and their key differences to ordinary autoencoding neural networks. As already motivated, variational autoencoders are generative models. This means, that they intend to capture the entire probability distribution of the observation space $Y$. Hence, they are statistical models and depend on certain probabilistic approaches.










In order to compute the gradient of a variational autoencoding neural network, we need to determine a way to quantify the distance between probability distributions. A popular measure for this case is the so called Kullback-Leibler divergence. It assesses the dissimilarity between two probability distributions over the same 	random variable $X$.
\begin{definition}\label{def_kl_div}
Let $X$ be a random variable and $p, q$ two probability density functions over $X$. Then, the \textbf{Kullback-Leibler divergence} is defined as
\begin{align}
\kldiv{p}{q} = \int p(x) \log \left( \frac{p(x)}{q(x)}\right) dx.
\end{align}
\end{definition}

If we consider the Kullback-Leibler divergence with regard to a dataset consisting of observed data samples, we can write equivalently.

\begin{definition}
Let $X$ be a random variable and $p, q$ two probability density functions over $X$. Furthermore, let $D=\{x_i\}_{i=i}^L$ be an unsupervised dataset of length $L\in\N$.\\
Then, the \textbf{Kullback-Leibler divergence} is defined as
\begin{align}
\kldiv{p}{q} = \sum_{i=1}^L p(x_i) \log \left( \frac{p(x_i)}{q(x_i)}\right).
\end{align}
\end{definition}

\begin{definition}\label{def_entropy}
Let $X$ be a random variable and $p$ a probability density function over $X$. Then, the nonnegative measure of the expected information content of $X$ under correct distribution $p$, defined as
\begin{align}
\H(X) = -\int p(x) \log p(x)dx = \E_p \left[ - \log p(x) \right],
\end{align}
is called \textbf{entropy} of $p$.
\end{definition}


\begin{definition}\label{def_cross_entropy}
Let $X$ be a random variable and $p, q$ two probability density functions over $X$. Then, the nonnegative measure of the expected information content of $X$ under incorrect distribution $q$, defined as
\begin{align}
\H_q(X) = -\int p(x) \log q(x)dx = \E_p \left[ - \log q(x) \right],
\end{align}
is called \textbf{cross-entropy} between $p$ and $q$.
\end{definition}


\begin{remark}
Let $X$ be a random variable and $p, q$ probability density functions over $X$. Then the Kullback-Leibler divergence between $p$ and $q$ can be written as follows
\begin{align*}
\kldiv{p}{q} = \H_q(p) - \H(p),
\end{align*}
where $\H_q$ and $\H$ denote the cross-entropy and entropy, respectively.
\end{remark}
