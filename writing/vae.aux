\relax
\providecommand\hyper@newdestlabel[2]{}
\citation{kingma2013auto}
\citation{cinelli2021variational}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Variational Autoencoders}{64}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:vae}{{3}{64}{Variational Autoencoders}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Probabilistic foundations}{64}{section.3.1}\protected@file@percent }
\newlabel{sec:prob_foundations}{{3.1}{64}{Probabilistic foundations}{section.3.1}{}}
\newlabel{def:cond_prob}{{3.1.1}{64}{}{theorem.3.1.1}{}}
\citation{klenke2013probability}
\citation{klenke2013probability}
\citation{cinelli2021variational}
\newlabel{theorem:bayes_rule}{{3.1.2}{66}{}{theorem.3.1.2}{}}
\newlabel{theorem:pdf_gen}{{3.1.3}{66}{}{theorem.3.1.3}{}}
\newlabel{ex:loc-scale_fam}{{3.1.4}{66}{}{theorem.3.1.4}{}}
\newlabel{eq:evidence}{{3.1}{67}{Probabilistic foundations}{equation.3.1.1}{}}
\newlabel{eq:posterior}{{3.2}{67}{Probabilistic foundations}{equation.3.1.2}{}}
\newlabel{def:KL_div}{{3.1.5}{67}{}{theorem.3.1.5}{}}
\citation{klenke2013probability}
\newlabel{def:KL_div_data}{{3.1.6}{68}{}{theorem.3.1.6}{}}
\newlabel{def:entropy}{{3.1.7}{68}{}{theorem.3.1.7}{}}
\newlabel{def:cross_entropy}{{3.1.8}{68}{}{theorem.3.1.8}{}}
\citation{kingma2013auto}
\citation{klenke2013probability}
\newlabel{eq:kl_entropy}{{3.7}{69}{Probabilistic foundations}{equation.3.1.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Variational Inference on Autoencoders}{69}{section.3.2}\protected@file@percent }
\newlabel{sec:vi_on_ae}{{3.2}{69}{Variational Inference on Autoencoders}{section.3.2}{}}
\newlabel{eq:likelihood_nn}{{3.8}{69}{Variational Inference on Autoencoders}{equation.3.2.8}{}}
\newlabel{eq:log_evidence}{{3.9}{69}{Variational Inference on Autoencoders}{equation.3.2.9}{}}
\citation{bishop2006pattern}
\newlabel{eq:elbo}{{3.10}{70}{Variational Inference on Autoencoders}{equation.3.2.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Applications}{71}{section.3.3}\protected@file@percent }
\newlabel{sec:vae_applications}{{3.3}{71}{Applications}{section.3.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Variational Autoencoder\relax }}{72}{algorithm.6}\protected@file@percent }
\newlabel{alg:general_vae}{{6}{72}{Variational Autoencoder\relax }{algorithm.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces On the left-hand side, the figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\lambda = \num {4e-5}$, where each dot is a sample from an encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded. On the right-hand side the figure illustrates the corresponding reconstruction through the variational autoencoder, where each point of the partitioned interval $[-8, 4]\times [-1, 10]$ is fed into the decoding architecture of the neural network to generate an image.\relax }}{73}{figure.caption.23}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_2D}{{3.1}{73}{On the left-hand side, the figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\l = \num {4e-5}$, where each dot is a sample from an encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded. On the right-hand side the figure illustrates the corresponding reconstruction through the variational autoencoder, where each point of the partitioned interval $[-8, 4]\times [-1, 10]$ is fed into the decoding architecture of the neural network to generate an image.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\lambda = \num {4e-5}$ to produce a reconstruction.\relax }}{73}{figure.caption.24}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_2D_inference}{{3.2}{73}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\l = \num {4e-5}$ to produce a reconstruction.\relax }{figure.caption.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The figure illustrates the training progresses of the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\lambda = \num {4e-5}$ optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis. On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }}{74}{figure.caption.25}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_2D_training_progress}{{3.3}{74}{The figure illustrates the training progresses of the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\l = \num {4e-5}$ optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis. On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-5}$ from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }}{75}{figure.caption.26}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_3D_latent}{{3.4}{75}{The figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-5}$ from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-5}$ to produce a reconstruction.\relax }}{75}{figure.caption.27}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_3D_inference}{{3.5}{75}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-5}$ to produce a reconstruction.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=64$ and KL-coefficient $\lambda = \num {4e-5}$ to produce a reconstruction.\relax }}{76}{figure.caption.28}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_64D_inference}{{3.6}{76}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=64$ and KL-coefficient $\l = \num {4e-5}$ to produce a reconstruction.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces On the left-hand side, the figure illustrates $100$ generated samples by the variational autoencoder with bottleneck $n_b=64$. On the right-hand side, the figure illustrates $100$ generated samples by the convolutional autoencoder witch bottleneck $n_b=64$ as well.\relax }}{76}{figure.caption.29}\protected@file@percent }
\newlabel{fig:convolutional_VAE_vs_convolutional_AE}{{3.7}{76}{On the left-hand side, the figure illustrates $100$ generated samples by the variational autoencoder with bottleneck $n_b=64$. On the right-hand side, the figure illustrates $100$ generated samples by the convolutional autoencoder witch bottleneck $n_b=64$ as well.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces The figure illustrates $100$ generated samples by the variational autoencoder with bottleneck $n_b=64$. To make the generation more stable, we first averaged the encoded means and variances over all digits in the dataset and afterwards applied some noise to the averaged mean.\relax }}{77}{figure.caption.30}\protected@file@percent }
\newlabel{fig:convolutional_VAE_better_generations}{{3.8}{77}{The figure illustrates $100$ generated samples by the variational autoencoder with bottleneck $n_b=64$. To make the generation more stable, we first averaged the encoded means and variances over all digits in the dataset and afterwards applied some noise to the averaged mean.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-4}$ from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }}{78}{figure.caption.31}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-4_5k_epochs_3D_latent}{{3.9}{78}{The figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-4}$ from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda =\num {4e-4}$ to produce a reconstruction.\relax }}{79}{figure.caption.32}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-4_5k_epochs_3D_inference}{{3.10}{79}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l =\num {4e-4}$ to produce a reconstruction.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces The figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-2}$ from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }}{79}{figure.caption.33}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-2_5k_epochs_3D_latent}{{3.11}{79}{The figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-2}$ from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda =\num {4e-2}$ to produce a reconstruction.\relax }}{80}{figure.caption.34}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-2_5k_epochs_3D_inference}{{3.12}{80}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l =\num {4e-2}$ to produce a reconstruction.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=10$ and KL-coefficient $\lambda =\num {4e-2}$ to produce a reconstruction.\relax }}{80}{figure.caption.35}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-2_10k_epochs_10D_inference}{{3.13}{80}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=10$ and KL-coefficient $\l =\num {4e-2}$ to produce a reconstruction.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=64$ and KL-coefficient $\lambda =\num {4e-2}$ to produce a reconstruction.\relax }}{81}{figure.caption.36}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-2_10k_epochs_64D_inference}{{3.14}{81}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=64$ and KL-coefficient $\l =\num {4e-2}$ to produce a reconstruction.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces The figure illustrates the latent space of the variational autoencoder with the posterior chosen customly, where we choose the bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-2}$, from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }}{81}{figure.caption.37}\protected@file@percent }
\newlabel{fig:convolutional_VAE_new_idea_KL_4e-2_10k_epochs_3D_latent}{{3.15}{81}{The figure illustrates the latent space of the variational autoencoder with the posterior chosen customly, where we choose the bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-2}$, from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with the posterior chosen customly, where we choose the bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-4}$ to produce a reconstruction.\relax }}{82}{figure.caption.38}\protected@file@percent }
\newlabel{fig:convolutional_VAE_new_idea_KL_4e-2_10k_epochs_3D_inference}{{3.16}{82}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with the posterior chosen customly, where we choose the bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-4}$ to produce a reconstruction.\relax }{figure.caption.38}{}}
\@setckpt{vae}{
\setcounter{page}{83}
\setcounter{equation}{10}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{16}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{12}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{13}
\setcounter{float@type}{8}
\setcounter{algorithm}{6}
\setcounter{ALG@line}{10}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{theorem}{0}
}
