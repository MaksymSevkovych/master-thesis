\relax
\providecommand\hyper@newdestlabel[2]{}
\citation{kingma2013auto}
\citation{cinelli2021variational}
\citation{klenke2013probability}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Variational Autoencoders}{65}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:vae}{{3}{65}{Variational Autoencoders}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Probabilistic foundations}{65}{section.3.1}\protected@file@percent }
\newlabel{sec:prob_foundations}{{3.1}{65}{Probabilistic foundations}{section.3.1}{}}
\newlabel{def:cond_prob}{{3.1.1}{65}{}{theorem.3.1.1}{}}
\citation{klenke2013probability}
\citation{cinelli2021variational}
\newlabel{theorem:bayes_rule}{{3.1.2}{66}{}{theorem.3.1.2}{}}
\newlabel{theorem:pdf_gen}{{3.1.3}{67}{}{theorem.3.1.3}{}}
\newlabel{ex:loc-scale_fam}{{3.1.4}{67}{}{theorem.3.1.4}{}}
\citation{klenke2013probability}
\newlabel{eq:evidence}{{3.1}{68}{Probabilistic foundations}{equation.3.1.1}{}}
\newlabel{eq:posterior}{{3.2}{68}{Probabilistic foundations}{equation.3.1.2}{}}
\newlabel{def:KL_div}{{3.1.5}{68}{}{theorem.3.1.5}{}}
\newlabel{def:KL_div_data}{{3.1.6}{68}{}{theorem.3.1.6}{}}
\citation{kingma2013auto}
\newlabel{def:entropy}{{3.1.7}{69}{}{theorem.3.1.7}{}}
\newlabel{def:cross_entropy}{{3.1.8}{69}{}{theorem.3.1.8}{}}
\newlabel{eq:kl_entropy}{{3.7}{69}{Probabilistic foundations}{equation.3.1.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Variational Inference on Autoencoders}{69}{section.3.2}\protected@file@percent }
\newlabel{sec:vi_on_ae}{{3.2}{69}{Variational Inference on Autoencoders}{section.3.2}{}}
\newlabel{eq:likelihood_nn}{{3.8}{69}{Variational Inference on Autoencoders}{equation.3.2.8}{}}
\citation{klenke2013probability}
\citation{bishop2006pattern}
\newlabel{eq:log_evidence}{{3.9}{70}{Variational Inference on Autoencoders}{equation.3.2.9}{}}
\newlabel{eq:elbo}{{3.10}{70}{Variational Inference on Autoencoders}{equation.3.2.10}{}}
\newlabel{eq:alternative_loss}{{3.11}{70}{Variational Inference on Autoencoders}{equation.3.2.11}{}}
\citation{kingma2013auto}
\citation{paisley2012variational}
\citation{kingma2013auto}
\newlabel{eq:loss_batched}{{3.12}{71}{Variational Inference on Autoencoders}{equation.3.2.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Applications}{72}{section.3.3}\protected@file@percent }
\newlabel{sec:vae_applications}{{3.3}{72}{Applications}{section.3.3}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6}{\ignorespaces Variational Autoencoder\relax }}{73}{algorithm.6}\protected@file@percent }
\newlabel{alg:general_vae}{{6}{73}{Variational Autoencoder\relax }{algorithm.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces On the left-hand side, the figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\lambda = \num {4e-5}$, where each dot is a sample from an encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded. On the right-hand side the figure illustrates the corresponding reconstruction through the variational autoencoder, where each point of the partitioned interval $[-4, 8]\times [-6, 6]$ is fed into the decoding architecture of the neural network to generate an image.\relax }}{74}{figure.caption.27}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_2D}{{3.1}{74}{On the left-hand side, the figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\l = \num {4e-5}$, where each dot is a sample from an encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded. On the right-hand side the figure illustrates the corresponding reconstruction through the variational autoencoder, where each point of the partitioned interval $[-4, 8]\times [-6, 6]$ is fed into the decoding architecture of the neural network to generate an image.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\lambda = \num {4e-5}$ to produce a reconstruction.\relax }}{74}{figure.caption.28}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_2D_inference}{{3.2}{74}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\l = \num {4e-5}$ to produce a reconstruction.\relax }{figure.caption.28}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The figure illustrates the training progresses of the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\lambda = \num {4e-5}$ with epochs on one axis and corresponding training loss on the other axis. On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }}{75}{figure.caption.29}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_2D_training_progress}{{3.3}{75}{The figure illustrates the training progresses of the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\l = \num {4e-5}$ with epochs on one axis and corresponding training loss on the other axis. On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\lambda = \num {4e-5}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }}{76}{figure.caption.30}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_2D_errors}{{3.4}{76}{The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=2$ and KL-coefficient $\l = \num {4e-5}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }{figure.caption.30}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-5}$ from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }}{76}{figure.caption.31}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_3D_latent}{{3.5}{76}{The figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-5}$ from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-5}$ to produce a reconstruction.\relax }}{77}{figure.caption.32}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_3D_inference}{{3.6}{77}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-5}$ to produce a reconstruction.\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-5}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }}{78}{figure.caption.33}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_3D_errors}{{3.7}{78}{The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-5}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }{figure.caption.33}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=64$ and KL-coefficient $\lambda = \num {4e-5}$ to produce a reconstruction.\relax }}{78}{figure.caption.34}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_64D_inference}{{3.8}{78}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=64$ and KL-coefficient $\l = \num {4e-5}$ to produce a reconstruction.\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=64$ and KL-coefficient $\lambda = \num {4e-5}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }}{79}{figure.caption.35}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-5_10k_epochs_64D_errors}{{3.9}{79}{The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=64$ and KL-coefficient $\l = \num {4e-5}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }{figure.caption.35}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces On the left-hand side, the figure illustrates $100$ generated samples by the variational autoencoder with bottleneck $n_b=64$. On the right-hand side, the figure illustrates $100$ generated samples by the convolutional autoencoder witch bottleneck $n_b=64$ as well.\relax }}{79}{figure.caption.36}\protected@file@percent }
\newlabel{fig:convolutional_VAE_vs_convolutional_AE}{{3.10}{79}{On the left-hand side, the figure illustrates $100$ generated samples by the variational autoencoder with bottleneck $n_b=64$. On the right-hand side, the figure illustrates $100$ generated samples by the convolutional autoencoder witch bottleneck $n_b=64$ as well.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces The figure illustrates $100$ generated samples by the variational autoencoder with bottleneck $n_b=64$. To make the generation more stable, we first averaged the encoded means and variances over all digits in the dataset and afterwards applied some noise to the averaged mean.\relax }}{80}{figure.caption.37}\protected@file@percent }
\newlabel{fig:convolutional_VAE_better_generations}{{3.11}{80}{The figure illustrates $100$ generated samples by the variational autoencoder with bottleneck $n_b=64$. To make the generation more stable, we first averaged the encoded means and variances over all digits in the dataset and afterwards applied some noise to the averaged mean.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces The figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-4}$ from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }}{81}{figure.caption.38}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-4_5k_epochs_3D_latent}{{3.12}{81}{The figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-4}$ from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda =\num {4e-4}$ to produce a reconstruction.\relax }}{81}{figure.caption.39}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-4_5k_epochs_3D_inference}{{3.13}{81}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l =\num {4e-4}$ to produce a reconstruction.\relax }{figure.caption.39}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-4}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }}{82}{figure.caption.40}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-4_5k_epochs_3D_errors}{{3.14}{82}{The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-4}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }{figure.caption.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces The figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-2}$ from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }}{82}{figure.caption.41}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-2_10k_epochs_3D_latent}{{3.15}{82}{The figure illustrates the latent space of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-2}$ from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda =\num {4e-2}$ to produce a reconstruction.\relax }}{83}{figure.caption.42}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-2_10k_epochs_3D_inference}{{3.16}{83}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l =\num {4e-2}$ to produce a reconstruction.\relax }{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-2}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }}{83}{figure.caption.43}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-2_10k_epochs_3D_errors}{{3.17}{83}{The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-2}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }{figure.caption.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=10$ and KL-coefficient $\lambda =\num {4e-2}$ to produce a reconstruction.\relax }}{84}{figure.caption.44}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-2_10k_epochs_10D_inference}{{3.18}{84}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=10$ and KL-coefficient $\l =\num {4e-2}$ to produce a reconstruction.\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=10$ and KL-coefficient $\lambda = \num {4e-2}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }}{84}{figure.caption.45}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-2_10k_epochs_10D_errors}{{3.19}{84}{The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=10$ and KL-coefficient $\l = \num {4e-2}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=64$ and KL-coefficient $\lambda =\num {4e-2}$ to produce a reconstruction.\relax }}{85}{figure.caption.46}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-2_10k_epochs_64D_inference}{{3.20}{85}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with bottleneck $n_b=64$ and KL-coefficient $\l =\num {4e-2}$ to produce a reconstruction.\relax }{figure.caption.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=64$ and KL-coefficient $\lambda = \num {4e-2}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }}{85}{figure.caption.47}\protected@file@percent }
\newlabel{fig:convolutional_VAE_snd_KL_4e-2_10k_epochs_64D_errors}{{3.21}{85}{The figure illustrates the test errors of the variational autoencoder with bottleneck $n_b=64$ and KL-coefficient $\l = \num {4e-2}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.22}{\ignorespaces The figure illustrates the latent space of the variational autoencoder with the posterior chosen customly, where we choose the bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-2}$, from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }}{86}{figure.caption.48}\protected@file@percent }
\newlabel{fig:convolutional_VAE_new_idea_KL_4e-2_10k_epochs_3D_latent}{{3.22}{86}{The figure illustrates the latent space of the variational autoencoder with the posterior chosen customly, where we choose the bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-2}$, from two different perspectives. Each dot is a sample from the encoded image of a digit onto a probability density. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.23}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with the posterior chosen customly, where we choose the bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-4}$ to produce a reconstruction.\relax }}{87}{figure.caption.49}\protected@file@percent }
\newlabel{fig:convolutional_VAE_new_idea_KL_4e-2_10k_epochs_3D_inference}{{3.23}{87}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with the posterior chosen customly, where we choose the bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-4}$ to produce a reconstruction.\relax }{figure.caption.49}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.24}{\ignorespaces The figure illustrates the test errors of the variational autoencoder with the posterior chosen customly with bottleneck $n_b=3$ and KL-coefficient $\lambda = \num {4e-2}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }}{87}{figure.caption.50}\protected@file@percent }
\newlabel{fig:convolutional_VAE_new_idea_KL_4e-2_10k_epochs_3D_errors}{{3.24}{87}{The figure illustrates the test errors of the variational autoencoder with the posterior chosen customly with bottleneck $n_b=3$ and KL-coefficient $\l = \num {4e-2}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }{figure.caption.50}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.25}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with the posterior chosen customly with bottleneck $n_b=10$ and KL-coefficient $\lambda =\num {4e-2}$ to produce a reconstruction.\relax }}{88}{figure.caption.51}\protected@file@percent }
\newlabel{fig:convolutional_VAE_new_idea_KL_4e-2_10k_epochs_10D_inference}{{3.25}{88}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with the posterior chosen customly with bottleneck $n_b=10$ and KL-coefficient $\l =\num {4e-2}$ to produce a reconstruction.\relax }{figure.caption.51}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.26}{\ignorespaces The figure illustrates the test errors of the variational autoencoder with the posterior chosen customly with bottleneck $n_b=10$ and KL-coefficient $\lambda = \num {4e-2}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }}{88}{figure.caption.52}\protected@file@percent }
\newlabel{fig:convolutional_VAE_new_idea_KL_4e-2_10k_epochs_10D_errors}{{3.26}{88}{The figure illustrates the test errors of the variational autoencoder with the posterior chosen customly with bottleneck $n_b=10$ and KL-coefficient $\l = \num {4e-2}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }{figure.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.27}{\ignorespaces On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with the posterior chosen customly with bottleneck $n_b=64$ and KL-coefficient $\lambda =\num {4e-2}$ to produce a reconstruction.\relax }}{89}{figure.caption.53}\protected@file@percent }
\newlabel{fig:convolutional_VAE_new_idea_KL_4e-2_10k_epochs_64D_inference}{{3.27}{89}{On the left-hand side, the figure illustrates $100$ original digits from the MNIST dataset. On the right-hand side, the figure illustrates the same digits after feeding them through the variational autoencoder with the posterior chosen customly with bottleneck $n_b=64$ and KL-coefficient $\l =\num {4e-2}$ to produce a reconstruction.\relax }{figure.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.28}{\ignorespaces The figure illustrates the test errors of the variational autoencoder with the posterior chosen customly with bottleneck $n_b=64$ and KL-coefficient $\lambda = \num {4e-2}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }}{89}{figure.caption.54}\protected@file@percent }
\newlabel{fig:convolutional_VAE_new_idea_KL_4e-2_10k_epochs_64D_errors}{{3.28}{89}{The figure illustrates the test errors of the variational autoencoder with the posterior chosen customly with bottleneck $n_b=64$ and KL-coefficient $\l = \num {4e-2}$, where each bar represents the averaged test errors over the entire MNIST dataset for each of the ten digits.\relax }{figure.caption.54}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.29}{\ignorespaces The figure illustrates $100$ generated samples by the variational autoencoder with bottleneck $n_b=64$ and customly chosen posterior. Where we applied some noise to the mean of the encoded densities.\relax }}{90}{figure.caption.55}\protected@file@percent }
\newlabel{fig:convolutional_VAE_better_generations_new_idea}{{3.29}{90}{The figure illustrates $100$ generated samples by the variational autoencoder with bottleneck $n_b=64$ and customly chosen posterior. Where we applied some noise to the mean of the encoded densities.\relax }{figure.caption.55}{}}
\@setckpt{vae}{
\setcounter{page}{91}
\setcounter{equation}{12}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{29}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{12}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{13}
\setcounter{float@type}{8}
\setcounter{algorithm}{6}
\setcounter{ALG@line}{10}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{theorem}{0}
}
