\justifying
\chapter{Well-posedness and strong convergence}

\section{Conditions}

Having defined the proposed network Tikhonov method by \eqref{Def_T}, we now want to consider its well-posedness. In order to do so, we need to introduce a bunch of assumptions.

\begin{assumption}\label{ass1}
\begin{outline}
\item[a)] For the \textbf{network regularizer} $\reg$ should hold:
\subitem The regularizer $\reg(\V,\wc)$ is defined by \eqref{Def_reg} and \eqref{Def_Phi}.
\subitem The affine operators $\V_l$ shall be given as in proposition \ref{prop1}.
\subitem The affine operators $\A_l$ are bounded linear.
\subitem The operators $\s_l$ are weakly continuous.
\subitem The functional $\psi$ is weakly lower-continuous.
\item[b)] For the \textbf{similarity measure} $\D$ should hold:
\subitem For some $\t \geq 1$ we have for all $ y_0,y_1,y_2 \in Y:\, \D(y_0,y_1)\leq \t\D(y_0,y_2) + \t\D(y_2,y_1)$.
\subitem For all $y_0,y_1 \in Y:\, \D(y_0,y_1) = 0 \,\Longleftrightarrow \, y_0=y_1$.
\subitem For all sequences $(y_k)_{k \in \N} \in Y^{\N} : y_k \to y \,\Longrightarrow \,\D(y_l,y) \to 0$.
\subitem The functional $(x,y) \mapsto \D(x,y)$ is sequentially lower semi-continuous.
\item[c)] A general \textbf{coercivity} condition should hold:
\subitem The regularizer $\reg(\V,\wc)$ is coercive, meaning $\reg(\V,x) \to \infty \Longrightarrow \|x\| \to \infty $.
\end{outline}

Here, the assumptions in $a)$ guarantee the lower semi-continuity of the regularizer, the assumptions in $b)$ guarantee data consistency and the assumption $c)$ will guranatee global convergence. We should note, that the last assumption is indeed the most restrictive.
\end{assumption}

\begin{remark}
Note, that the coercivity condition $c)$ in assumption \ref{ass1} can be easily obtained by defining the regularizer $\reg(\V,\wc)$ by
\begin{align*}
\forall x \in D: \quad \reg(\V,x) \coloneqq \reg^{(1)}(\V,x) + \psi^{(2)}(x),
\end{align*}
where $\reg^{(1)}(\V,\wc)$ is a pre-trained regularizer satisfying assumption \ref{ass1} $a)$ and $\psi^{(2)}(\wc)$ is a coercive, weakly lower semi-continuous functional.
\end{remark}

\section{Well-posedness}

With the introduced conditions in assumption \ref{ass1} we can propose a theorem showing the well-posedness and convergence of the proposed NETT method.

\begin{theorem}\label{theorem1}
Let assumption \ref{ass1} be satisfied. Then the following assertions hold true:
\begin{itemize}
\item[a)]\textbf{Existence:} For all $y \in Y$ and $\a > 0$ there exists a minimizer $\T_{\a;y}$.
\vspace{0.3cm}
\item[b)]\textbf{Stability:} If $y_k \to y$ and $x_k \in \argmin_{x\in D}\{\T_{\a;y_k}(x)\}$ then weak accumulation points of $(x_k)_{k \in \N}$ exist and are minimizers of $\T_{\a;y}$.
\vspace{0.3cm}
\item[c)]\textbf{Convergence:} Let $x \in X, \, y = \F(x)$ and the sequence $(y_k)_{k \in \N}$ in $Y^{\N}$ satisfy\\ $\D(y_k,y), \D(y,y_k)\leq \d_k$ for some sequence $(\d_{k})_{k \in \N} \subset (0,\infty)^{\N}$ with $\d_{k}\to 0$.\\ Furthermore, suppose $(x_k)_{k\in\N} \subset \argmin_{x\in D}\{\T_{\a(\d_k);y_k}(x)\}$ and let the parameter $\a$ satisfy
\begin{align}\label{param}
\a: (0,\infty) \to (0, \infty), \quad \lim_{\d \to 0} \a(\d) = \lim_{\d \to 0} \frac{\d}{\a(\d)} = 0.
\end{align}
Then the following holds:
\begin{itemize}
\item[i)] Weak accumulation points of $(x_k)_{k \in\N} \subset D^{\N}$ are solutions of $\F(x)=y$, minimizing the regularizer $\reg(\V,\wc)$.
\item[ii)] The sequence $(x_k)_{k\in\N} \subset D^{\N}$ has at least one weak accumulation point $x_+$ in\\ $\argmin\{\reg(\V,x)\,|\,x \in D \wedge \F(x) = \displaystyle\lim_{\d \to 0}y_{\d}\}$.
\item[iii)] Any subsequence $(x_{k_n})_{n\in\N}$ weakly converging to an $x_+$ satisfies $\reg(\V,x_{k_n})\to \reg(\V,x_+)$.
\item[iv)] If the solution of $\F(x) = y$, minimizing $\reg(\V,\wc)$ is unique, then $x_k \rightharpoonup x_+$.
\end{itemize}
\end{itemize}
\end{theorem}

\begin{proof}
Since this proof is very technical and based on multiple results proven by the authors in prior papers, we will not perform it here. Instead, we refer to the works \cite{li2020nett, grasmair2010generalized, scherzer2009variational}.
\end{proof}

Now, we want to tackle the strong convergence of our proposed NETT method. In order to do that we first need to define two quantities, the absolute Bregman distance and the modulus of total non-linearity. We need these quantities to generalize our results from a convex to a non-convex setting.

\begin{definition}\label{Def_absbreg}
Let $\mathcal{F}: D\subseteq X \to \R$ be G\^ateaux differentiable at $x \in X$. Then the \textbf{absolute Bregman distance} $\absbreg(\wc,x): D \to [0,\infty]$ with respect to $\mathcal{F}$ at $x$ is defined by
\begin{align*}
\forall \bar{x} \in X: \quad \absbreg (\bar{x},x) \coloneqq \left|\mathcal{F}(\bar{x}) -\mathcal{F}(x) - \mathcal{F}^\prime(x)(\bar{x} - x)\right|,
\end{align*}
where $\mathcal{F}^\prime(x)$ denotes the G\^ateaux derivative of $\mathcal{F}$ at $x\in X$.
\end{definition}

\begin{definition}\label{Def_tnl}
Let $\mathcal{F}: D\subseteq X \to \R$ be G\^ateaux differentiable at $x \in D$. Then the \textbf{modulus of total non-linearity} of $\mathcal{F}$ at $x$ is defined as $\tnl(x,\wc): [0,\infty) \to [0, \infty]$:
\begin{align*}
\forall t > 0: \quad \tnl(x,t)\coloneqq \inf \left\{\absbreg(\bar{x}, x) \,|\, \bar{x} \in D \wedge \|\bar{x} - x \| = t \right\},
\end{align*}
where the function $\mathcal{F}$ is called \textbf{totally non-linear} at $x$, if $\tnl(x,t) > 0$ for all $t \in (0, \infty)$.
\end{definition}

\newpage
With the definitions above we can introduce the following result, cited from \cite{resmerita2004total}[proposition 2.2]

\begin{proposition}\label{prop2}
For $\mathcal{F}: D \subseteq X \to \R$ and arbitrary but fixed $x \in D$ the following assertions are equivalent:
\begin{enumerate}
\item[a)] The function $\mathcal{F}$ is totally non-linear at $x$.
\item[b)] For all sequences $(x_k)_{k \in\N} \subset D^{\N}$ satisfying $\lim_{n \to \infty}\absbreg(x_n,x) = 0$ and $(x_n)_{n\in\N}$ bounded, it holds that $\lim_{n\to\infty} \|x_n - x\| = 0$.
\end{enumerate}
\end{proposition}

\begin{proof}
Let $a)$ hold. Suppose for all sequences $(x_k)_{k \in\N} \subset D^{\N}$ satisfying $\lim_{n \to \infty}\absbreg(x_n,x) = 0$ and $(x_n)_{n\in\N}$ bounded, it holds that $\lim_{n\to\infty} \|x_n - x\| = \d > 0$. Since $\absbreg(\wc,x)$ is continuous, there exists for any $\varepsilon > 0$ an $\bar{x}_n$ satisfying $\|\bar{x}_n - x\| = \d$ such that for sufficiently large $n \in\N$
\begin{align*}
\varepsilon \geq \absbreg(x_n,x) + \frac{\varepsilon}{2} \geq \absbreg(\bar{x}_n,x) \geq \tnl(x,\d).
\end{align*}
Since this inequality holds for all $\varepsilon > 0$ it follows that $\tnl(x,\d)=0$, which contradicts the total non-linearity of $\mathcal{F}$ at $x$. Hence, the implication $"a) \Longrightarrow b)"$ is proven by contradiction and it follows that $\lim_{n\to\infty} \|x_n - x\| = 0$.\\
The second implication $"b) \Longrightarrow a)"$ is stated in \cite{resmerita2004total}[proposition 2.2].
\end{proof}

\begin{remark}
Note, that proposition \ref{prop2} remains true, if we use any non-negative continuous function $\f: \R \to [0, \infty]$ that satisfies $\f(0) = 0$ (e.g.: the rectified linear unit (ReLU) $\s: \R \to [0,\infty],\, x \mapsto \max\{x, 0\}$) instead of the absolute distance in the definition of the Bregman distance \ref{Def_absbreg}.
\end{remark}

\section{Strong convergence}

Using the previous results we now can prove the strong convergence of the network Tikhonov regularization method.

\begin{theorem}\label{theorem2}
Let assumption \ref{ass1} hold and assume additionally that there exists a solution of $\F(x) = y$. Let the regularizer $\reg(\V,\wc)$ be totally non-linear at its minimizing solutions and the parameter $\a$ satisfy \eqref{param}.\\
Then there exists for every sequence $(y_k)_{k\in\N} \subset Y^{\N}$ satisfying $\D(y_k,y), \D(y,y_k)\leq \d_k$ where $\d_k \to 0$ and every sequence $(x_k)_{k\in\N} \subset \argmin_{x\in D}\{\T_{\a(\d_k);y_k}(x)\}$ a subsequence $(x_{k_n})_{n\in\N}$ and a $\reg(\V,\wc)$-minimizing solution $x_+$ with $\|x_{k_n} - x_+\|\to 0$.\\
Furthermore, if $x_+$ is the unique solution minimizing $\reg(\V,\wc)$, then $x_k \to x_+$ with respect to the norm topology.
\end{theorem}

\begin{proof}
From Theorem \ref{theorem1} we obtain the existence of a subsequence $(x_{k_n})_{n\in\N}$ satisfying $x_{k_n} \rightharpoonup x_+$ such that $\reg(\V,x_{k_n}) \to \reg(\V,x_+)$. Hence, from choosing $\mathcal{F}(\wc) = \reg(\V,\wc)$ and the convergence of $(\reg(\V,x_{k_n}))_{n\in\N}$ it follows, that the absolute Bregman distance converges as well $\mathcal{B}_{\reg(\V,\wc)}(x_{k_n},x_+) \to 0$. Additionally, we should note that the regularizer is bounded by Assumption \ref{ass1}, what directly follows from the boundedness of $\A_l$. Therefore, plugging these considerations into proposition \ref{prop2} yields $\lim_{n\to \infty} \|x_{k_n} - x_+\| = 0$.\\
Lastly, if the solution $x_+$ that minimizes the regularizer $\reg(\V,\wc)$ is unique, we even obtain the strong convergence $x_k \to x_+$.
\end{proof}
