\relax
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Autoencoders}{35}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:ae}{{2}{35}{Autoencoders}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Conceptional ideas}{35}{section.2.1}\protected@file@percent }
\newlabel{def_encoder}{{2.1.1}{35}{}{theorem.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces An autoencoding neural network with input and output $x, y\in \R  ^5$. The five hidden layers have dimensions $4$, $3$, $2$, $3$ and $4$ respectively. Hence, the bottleneck dimension is $2$ in this example. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }}{36}{figure.caption.4}\protected@file@percent }
\newlabel{fig:autoencoder}{{2.1}{36}{An autoencoding neural network with input and output $x, y\in \R ^5$. The five hidden layers have dimensions $4$, $3$, $2$, $3$ and $4$ respectively. Hence, the bottleneck dimension is $2$ in this example. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces An encoding neural network with input $x\in \R  ^5$ and output $y \in \R  ^2$. The two hidden layers have dimensions $4$ and $3$. Hence, the encoder reduces the data dimensionality from $5$ to $2$ dimension. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }}{36}{figure.caption.5}\protected@file@percent }
\newlabel{img_encoder}{{2.2}{36}{An encoding neural network with input $x\in \R ^5$ and output $y \in \R ^2$. The two hidden layers have dimensions $4$ and $3$. Hence, the encoder reduces the data dimensionality from $5$ to $2$ dimension. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A decoding neural network with input $x\in \R  ^2$ and output $y \in \R  ^5$. The two hidden layers have dimensions $3$ and $4$. Hence, the decoder expands the data dimensionality from $2$ to $5$ dimensions. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }}{37}{figure.caption.6}\protected@file@percent }
\newlabel{img_decoder}{{2.3}{37}{A decoding neural network with input $x\in \R ^2$ and output $y \in \R ^5$. The two hidden layers have dimensions $3$ and $4$. Hence, the decoder expands the data dimensionality from $2$ to $5$ dimensions. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }{figure.caption.6}{}}
\newlabel{def_decoder}{{2.1.2}{37}{}{theorem.2.1.2}{}}
\newlabel{lemma:composition_of_nns}{{2.1.3}{37}{}{theorem.2.1.3}{}}
\newlabel{nn_1}{{2.1}{37}{Conceptional ideas}{equation.2.1.1}{}}
\newlabel{nn_2}{{2.2}{37}{Conceptional ideas}{equation.2.1.2}{}}
\newlabel{nn_comp}{{2.3}{38}{Conceptional ideas}{equation.2.1.3}{}}
\citation{foster2022generative}
\newlabel{def_autoencoder}{{2.1.5}{39}{}{theorem.2.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Training of autoencoders}{39}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Applications}{40}{section.2.3}\protected@file@percent }
\newlabel{def:linear_encoder}{{2.3.1}{40}{}{theorem.2.3.1}{}}
\newlabel{def:linear_decoder}{{2.3.2}{40}{}{theorem.2.3.2}{}}
\newlabel{def:mnist}{{2.3.4}{41}{}{theorem.2.3.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Linear Autoencoder\relax }}{41}{algorithm.4}\protected@file@percent }
\newlabel{alg:linear_AE}{{4}{41}{Linear Autoencoder\relax }{algorithm.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces On the left side, the figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=2$ optimized with an Adam optimizer, where each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded. On the right side the figure illustrates the corresponding reconstruction through the autoencoder. Each coordinate tuple is fed into the decoding architecture of the autoencoder to generate an image.\relax }}{42}{figure.caption.7}\protected@file@percent }
\newlabel{fig:linear_AE_2d_adam_latent}{{2.4}{42}{On the left side, the figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=2$ optimized with an Adam optimizer, where each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded. On the right side the figure illustrates the corresponding reconstruction through the autoencoder. Each coordinate tuple is fed into the decoding architecture of the autoencoder to generate an image.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces On the left side, the figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=2$ optimized with an AMSGrad optimizer, where each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded. On the right side the figure illustrates the corresponding reconstruction through the autoencoder. Each coordinate tuple is fed into the decoding architecture of the autoencoder to generate an image.\relax }}{42}{figure.caption.8}\protected@file@percent }
\newlabel{fig:linear_AE_2d_amsgrad_latent}{{2.5}{42}{On the left side, the figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=2$ optimized with an AMSGrad optimizer, where each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded. On the right side the figure illustrates the corresponding reconstruction through the autoencoder. Each coordinate tuple is fed into the decoding architecture of the autoencoder to generate an image.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The figure illustrates the inference of the linear autoencoder with bottleneck $n_b=2$ optimized with an Adam optimizer (left) and an AMSGrad optimizer (right). The inference are ten reconstructed images for each of the ten digits.\relax }}{43}{figure.caption.9}\protected@file@percent }
\newlabel{fig:linear_AE_2d_inference}{{2.6}{43}{The figure illustrates the inference of the linear autoencoder with bottleneck $n_b=2$ optimized with an Adam optimizer (left) and an AMSGrad optimizer (right). The inference are ten reconstructed images for each of the ten digits.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=2$ optimized with an Adam optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }}{43}{figure.caption.10}\protected@file@percent }
\newlabel{fig:linear_AE_2d_adam_training_progress}{{2.7}{43}{The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=2$ optimized with an Adam optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=2$ optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }}{44}{figure.caption.11}\protected@file@percent }
\newlabel{fig:linear_AE_2d_amsgrad_training_progress}{{2.8}{44}{The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=2$ optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=3$ optimized with an Adam optimizer from two different perspectives. Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }}{44}{figure.caption.12}\protected@file@percent }
\newlabel{fig:linear_AE_3d_adam_latent}{{2.9}{44}{The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=3$ optimized with an Adam optimizer from two different perspectives. Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=3$ optimized with an AMSGrad optimizer from two different perspectives. Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }}{45}{figure.caption.13}\protected@file@percent }
\newlabel{fig:linear_AE_3d_amsgrad_latent}{{2.10}{45}{The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=3$ optimized with an AMSGrad optimizer from two different perspectives. Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces The figure illustrates the inference of the linear autoencoder with bottleneck $n_b=3$ optimized with an Adam optimizer (left) and an AMSGrad optimizer (right). The inference are ten reconstructed images for each of the ten digits\relax }}{45}{figure.caption.14}\protected@file@percent }
\newlabel{fig:linear_AE_3d_inference}{{2.11}{45}{The figure illustrates the inference of the linear autoencoder with bottleneck $n_b=3$ optimized with an Adam optimizer (left) and an AMSGrad optimizer (right). The inference are ten reconstructed images for each of the ten digits\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=3$ optimized with an Adam optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }}{46}{figure.caption.15}\protected@file@percent }
\newlabel{fig:linear_AE_3d_adam_training_progress}{{2.12}{46}{The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=3$ optimized with an Adam optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=3$ optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }}{46}{figure.caption.16}\protected@file@percent }
\newlabel{fig:linear_AE_3d_amsgrad_training_progress}{{2.13}{46}{The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=3$ optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }{figure.caption.16}{}}
\newlabel{def_convolutional_encoder}{{2.3.5}{47}{}{theorem.2.3.5}{}}
\newlabel{def_convolutional_decoder}{{2.3.6}{47}{}{theorem.2.3.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Convolutional Autoencoder\relax }}{48}{algorithm.5}\protected@file@percent }
\newlabel{alg:convolutional_AE}{{5}{48}{Convolutional Autoencoder\relax }{algorithm.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces The figure illustrates the activations of all $64$ channels in the latent space of our trained convolutional autoencoder, which was optimized with an AMSGrad optimizer, see Algorithm \ref {alg:convolutional_AE}.\relax }}{49}{figure.caption.17}\protected@file@percent }
\newlabel{fig:convolutional_AE_latent}{{2.14}{49}{The figure illustrates the activations of all $64$ channels in the latent space of our trained convolutional autoencoder, which was optimized with an AMSGrad optimizer, see Algorithm \ref {alg:convolutional_AE}.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces The figure illustrates the inference of the convolutional autoencoder, optimized with the AMSGrad optimizer. The inference consists of the reconstructed images for each of the ten digits.\relax }}{50}{figure.caption.18}\protected@file@percent }
\newlabel{fig:convolutional_AE_inference}{{2.15}{50}{The figure illustrates the inference of the convolutional autoencoder, optimized with the AMSGrad optimizer. The inference consists of the reconstructed images for each of the ten digits.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces The figure illustrates the training progresses of the convolutional autoencoder optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis. On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line every $300$ epochs to point out the trend of the training progress.\relax }}{50}{figure.caption.19}\protected@file@percent }
\newlabel{fig:convolutional_AE_training_progress}{{2.16}{50}{The figure illustrates the training progresses of the convolutional autoencoder optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis. On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line every $300$ epochs to point out the trend of the training progress.\relax }{figure.caption.19}{}}
\@setckpt{autoencoders}{
\setcounter{page}{51}
\setcounter{equation}{3}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{16}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{12}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{5}
\setcounter{ALG@line}{8}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{theorem}{7}
}
