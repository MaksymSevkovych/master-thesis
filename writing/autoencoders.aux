\relax
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Autoencoders}{17}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Conceptional ideas}{17}{section.2.1}\protected@file@percent }
\newlabel{def_encoder}{{2.1.1}{17}{}{theorem.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces An autoencoding neural network with input and output $x, y\in \R  ^5$. The five hidden layers have dimensions $4$, $3$, $2$, $3$ and $4$ respectively. Hence, the bottleneck dimension is $2$ in this example. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }}{18}{figure.caption.3}\protected@file@percent }
\newlabel{autoencoder}{{2.1}{18}{An autoencoding neural network with input and output $x, y\in \R ^5$. The five hidden layers have dimensions $4$, $3$, $2$, $3$ and $4$ respectively. Hence, the bottleneck dimension is $2$ in this example. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces An encoding neural network with input $x\in \R  ^5$ and output $y \in \R  ^2$. The two hidden layers have dimensions $4$ and $3$. Hence, the encoder reduces the data dimensionality from $5$ to $2$ dimension. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }}{18}{figure.caption.4}\protected@file@percent }
\newlabel{img_encoder}{{2.2}{18}{An encoding neural network with input $x\in \R ^5$ and output $y \in \R ^2$. The two hidden layers have dimensions $4$ and $3$. Hence, the encoder reduces the data dimensionality from $5$ to $2$ dimension. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A decoding neural network with input $x\in \R  ^2$ and output $y \in \R  ^5$. The two hidden layers have dimensions $3$ and $4$. Hence, the decoder expands the data dimensionality from $2$ to $5$ dimensions. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }}{19}{figure.caption.5}\protected@file@percent }
\newlabel{img_decoder}{{2.3}{19}{A decoding neural network with input $x\in \R ^2$ and output $y \in \R ^5$. The two hidden layers have dimensions $3$ and $4$. Hence, the decoder expands the data dimensionality from $2$ to $5$ dimensions. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }{figure.caption.5}{}}
\newlabel{def_decoder}{{2.1.2}{19}{}{theorem.2.1.2}{}}
\newlabel{lemma:composition_of_nns}{{2.1.3}{19}{}{theorem.2.1.3}{}}
\newlabel{nn_1}{{2.1}{19}{Conceptional ideas}{equation.2.1.1}{}}
\newlabel{nn_2}{{2.2}{19}{Conceptional ideas}{equation.2.1.2}{}}
\newlabel{nn_comp}{{2.3}{20}{Conceptional ideas}{equation.2.1.3}{}}
\citation{foster2022generative}
\newlabel{def_autoencoder}{{2.1.5}{21}{}{theorem.2.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Training of autoencoders}{21}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Applications}{22}{section.2.3}\protected@file@percent }
\newlabel{def_linear_encoder}{{2.3.1}{22}{}{theorem.2.3.1}{}}
\newlabel{def_linear_decoder}{{2.3.2}{22}{}{theorem.2.3.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=2$ optimized with an ADAM optimizer (left) and an AMSGrad optimizer (right). Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }}{23}{figure.caption.6}\protected@file@percent }
\newlabel{fig:linear_AE_2d_latent}{{2.4}{23}{The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=2$ optimized with an ADAM optimizer (left) and an AMSGrad optimizer (right). Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.6}{}}
\newlabel{def:mnist}{{2.3.4}{23}{}{theorem.2.3.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Linear Autoencoder\relax }}{23}{algorithm.4}\protected@file@percent }
\newlabel{alg:linear_AE_2d}{{4}{23}{Linear Autoencoder\relax }{algorithm.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The figure illustrates the inference of the linear autoencoder with bottleneck $n_b=2$ optimized with an ADAM optimizer (left) and an AMSGrad optimizer (right). The inference are ten reconstructed images for each of the ten digits.\relax }}{24}{figure.caption.7}\protected@file@percent }
\newlabel{fig:linear_AE_2d_inference}{{2.5}{24}{The figure illustrates the inference of the linear autoencoder with bottleneck $n_b=2$ optimized with an ADAM optimizer (left) and an AMSGrad optimizer (right). The inference are ten reconstructed images for each of the ten digits.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=2$ optimized with an ADAM optimizer (left) and an AMSGrad optimizer (right) with epochs on one axis and corresponding training loss on the other axis.\relax }}{24}{figure.caption.8}\protected@file@percent }
\newlabel{fig:linear_AE_2d_training_progress}{{2.6}{24}{The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=2$ optimized with an ADAM optimizer (left) and an AMSGrad optimizer (right) with epochs on one axis and corresponding training loss on the other axis.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=3$ optimized with an ADAM optimizer from two different perspectives. Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }}{25}{figure.caption.9}\protected@file@percent }
\newlabel{fig:linear_AE_3d_adam_latent}{{2.7}{25}{The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=3$ optimized with an ADAM optimizer from two different perspectives. Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=3$ optimized with an AMSGrad optimizer from two different perspectives. Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }}{25}{figure.caption.10}\protected@file@percent }
\newlabel{fig:linear_AE_3d_amsgrad_latent}{{2.8}{25}{The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=3$ optimized with an AMSGrad optimizer from two different perspectives. Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The figure illustrates the inference of the linear autoencoder with bottleneck $n_b=3$ optimized with an ADAM optimizer (left) and an AMSGrad optimizer (right). The inference are ten reconstructed images for each of the ten digits\relax }}{26}{figure.caption.11}\protected@file@percent }
\newlabel{fig:linear_AE_3d_inference}{{2.9}{26}{The figure illustrates the inference of the linear autoencoder with bottleneck $n_b=3$ optimized with an ADAM optimizer (left) and an AMSGrad optimizer (right). The inference are ten reconstructed images for each of the ten digits\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=3$ optimized with an ADAM optimizer (left) and an AMSGrad optimizer (right) with epochs on one axis and corresponding training loss on the other axis.\relax }}{26}{figure.caption.12}\protected@file@percent }
\newlabel{fig:linear_AE_3d_training_progress}{{2.10}{26}{The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=3$ optimized with an ADAM optimizer (left) and an AMSGrad optimizer (right) with epochs on one axis and corresponding training loss on the other axis.\relax }{figure.caption.12}{}}
\@setckpt{autoencoders}{
\setcounter{page}{27}
\setcounter{equation}{3}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{2}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{4}
\setcounter{ALG@line}{9}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{theorem}{4}
}
