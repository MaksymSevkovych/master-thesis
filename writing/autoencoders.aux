\relax
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Autoencoders}{45}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:ae}{{2}{45}{Autoencoders}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Conceptional ideas}{45}{section.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces An autoencoding neural network with input and output $x, y\in \R  ^5$. The five hidden layers have dimensions $4$, $3$, $2$, $3$ and $4$ respectively. Hence, the bottleneck dimension is $2$ in this example. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }}{46}{figure.caption.3}\protected@file@percent }
\newlabel{fig:autoencoder}{{2.1}{46}{An autoencoding neural network with input and output $x, y\in \R ^5$. The five hidden layers have dimensions $4$, $3$, $2$, $3$ and $4$ respectively. Hence, the bottleneck dimension is $2$ in this example. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }{figure.caption.3}{}}
\newlabel{def_encoder}{{2.1.1}{46}{}{theorem.2.1.1}{}}
\newlabel{def_decoder}{{2.1.2}{46}{}{theorem.2.1.2}{}}
\newlabel{lemma:composition_of_nns}{{2.1.3}{46}{}{theorem.2.1.3}{}}
\newlabel{nn_1}{{2.1}{46}{Conceptional ideas}{equation.2.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces An encoding neural network with input $x\in \R  ^5$ and output $y \in \R  ^2$. The two hidden layers have dimensions $4$ and $3$. Hence, the encoder reduces the data dimensionality from $5$ to $2$ dimension. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }}{47}{figure.caption.4}\protected@file@percent }
\newlabel{img_encoder}{{2.2}{47}{An encoding neural network with input $x\in \R ^5$ and output $y \in \R ^2$. The two hidden layers have dimensions $4$ and $3$. Hence, the encoder reduces the data dimensionality from $5$ to $2$ dimension. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces A decoding neural network with input $x\in \R  ^2$ and output $y \in \R  ^5$. The two hidden layers have dimensions $3$ and $4$. Hence, the decoder expands the data dimensionality from $2$ to $5$ dimensions. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }}{48}{figure.caption.5}\protected@file@percent }
\newlabel{img_decoder}{{2.3}{48}{A decoding neural network with input $x\in \R ^2$ and output $y \in \R ^5$. The two hidden layers have dimensions $3$ and $4$. Hence, the decoder expands the data dimensionality from $2$ to $5$ dimensions. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }{figure.caption.5}{}}
\newlabel{nn_2}{{2.2}{49}{Conceptional ideas}{equation.2.1.2}{}}
\newlabel{nn_comp}{{2.3}{49}{Conceptional ideas}{equation.2.1.3}{}}
\citation{foster2022generative}
\newlabel{def_autoencoder}{{2.1.5}{50}{}{theorem.2.1.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Training of autoencoders}{50}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Applications}{51}{section.2.3}\protected@file@percent }
\newlabel{sec:applications}{{2.3}{51}{Applications}{section.2.3}{}}
\newlabel{def:linear_encoder}{{2.3.1}{51}{}{theorem.2.3.1}{}}
\newlabel{def:linear_decoder}{{2.3.2}{51}{}{theorem.2.3.2}{}}
\newlabel{remark:mnist}{{2.3.4}{52}{}{theorem.2.3.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Linear Autoencoder\relax }}{52}{algorithm.4}\protected@file@percent }
\newlabel{alg:linear_AE}{{4}{52}{Linear Autoencoder\relax }{algorithm.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces On the left side, the figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=2$ optimized with an Adam optimizer, where each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded. On the right side the figure illustrates the corresponding reconstruction through the autoencoder. Each coordinate tuple is fed into the decoding architecture of the autoencoder to generate an image.\relax }}{54}{figure.caption.6}\protected@file@percent }
\newlabel{fig:linear_AE_2d_adam_latent}{{2.4}{54}{On the left side, the figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=2$ optimized with an Adam optimizer, where each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded. On the right side the figure illustrates the corresponding reconstruction through the autoencoder. Each coordinate tuple is fed into the decoding architecture of the autoencoder to generate an image.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces On the left side, the figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=2$ optimized with an AMSGrad optimizer, where each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded. On the right side the figure illustrates the corresponding reconstruction through the autoencoder. Each coordinate tuple is fed into the decoding architecture of the autoencoder to generate an image.\relax }}{54}{figure.caption.7}\protected@file@percent }
\newlabel{fig:linear_AE_2d_amsgrad_latent}{{2.5}{54}{On the left side, the figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=2$ optimized with an AMSGrad optimizer, where each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded. On the right side the figure illustrates the corresponding reconstruction through the autoencoder. Each coordinate tuple is fed into the decoding architecture of the autoencoder to generate an image.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces On the left side, the figure illustrates $100$ original digits from the MNIST dataset. On the right side, the figure illustrates the same digits after feeding them through the linear autoencoder with bottleneck $n_b=2$ optimized with an Adam optimizer.\relax }}{55}{figure.caption.8}\protected@file@percent }
\newlabel{fig:linear_AE_2d_adam_inference}{{2.6}{55}{On the left side, the figure illustrates $100$ original digits from the MNIST dataset. On the right side, the figure illustrates the same digits after feeding them through the linear autoencoder with bottleneck $n_b=2$ optimized with an Adam optimizer.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces On the left side, the figure illustrates $100$ original digits from the MNIST dataset. On the right side, the figure illustrates the same digits after feeding them through the linear autoencoder with bottleneck $n_b=2$ optimized with an AMSGrad optimizer.\relax }}{55}{figure.caption.9}\protected@file@percent }
\newlabel{fig:linear_AE_2d_amsgrad_inference}{{2.7}{55}{On the left side, the figure illustrates $100$ original digits from the MNIST dataset. On the right side, the figure illustrates the same digits after feeding them through the linear autoencoder with bottleneck $n_b=2$ optimized with an AMSGrad optimizer.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=2$ optimized with an Adam optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }}{56}{figure.caption.10}\protected@file@percent }
\newlabel{fig:linear_AE_2d_adam_training_progress}{{2.8}{56}{The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=2$ optimized with an Adam optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=2$ optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }}{56}{figure.caption.11}\protected@file@percent }
\newlabel{fig:linear_AE_2d_amsgrad_training_progress}{{2.9}{56}{The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=2$ optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=3$ optimized with an Adam optimizer from two different perspectives. Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }}{57}{figure.caption.12}\protected@file@percent }
\newlabel{fig:linear_AE_3d_adam_latent}{{2.10}{57}{The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=3$ optimized with an Adam optimizer from two different perspectives. Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.12}{}}
\newlabel{def_convolutional_encoder}{{2.3.5}{57}{}{theorem.2.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=3$ optimized with an AMSGrad optimizer from two different perspectives. Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }}{58}{figure.caption.13}\protected@file@percent }
\newlabel{fig:linear_AE_3d_amsgrad_latent}{{2.11}{58}{The figure illustrates the latent space of the linear autoencoder with bottleneck $n_b=3$ optimized with an AMSGrad optimizer from two different perspectives. Each dot is one encoded image of a digit. The color and the corresponding color map represent the digit that was encoded.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces On the left side, the figure illustrates $100$ original digits from the MNIST dataset. On the right side, the figure illustrates the same digits after feeding them through the linear autoencoder with bottleneck $n_b=3$ optimized with an Adam optimizer.\relax }}{58}{figure.caption.14}\protected@file@percent }
\newlabel{fig:linear_AE_3d_adam_inference}{{2.12}{58}{On the left side, the figure illustrates $100$ original digits from the MNIST dataset. On the right side, the figure illustrates the same digits after feeding them through the linear autoencoder with bottleneck $n_b=3$ optimized with an Adam optimizer.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces On the left side, the figure illustrates $100$ original digits from the MNIST dataset. On the right side, the figure illustrates the same digits after feeding them through the linear autoencoder with bottleneck $n_b=3$ optimized with an AMSGrad optimizer.\relax }}{59}{figure.caption.15}\protected@file@percent }
\newlabel{fig:linear_AE_3d_amsgrad_inference}{{2.13}{59}{On the left side, the figure illustrates $100$ original digits from the MNIST dataset. On the right side, the figure illustrates the same digits after feeding them through the linear autoencoder with bottleneck $n_b=3$ optimized with an AMSGrad optimizer.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=3$ optimized with an Adam optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }}{59}{figure.caption.16}\protected@file@percent }
\newlabel{fig:linear_AE_3d_adam_training_progress}{{2.14}{59}{The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=3$ optimized with an Adam optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=3$ optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }}{60}{figure.caption.17}\protected@file@percent }
\newlabel{fig:linear_AE_3d_amsgrad_training_progress}{{2.15}{60}{The figure illustrates the training progresses of the linear autoencoder with bottleneck $n_b=3$ optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis.On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line represents the moving average over $100$ epochs to point out the trend of the training progress.\relax }{figure.caption.17}{}}
\newlabel{def_convolutional_decoder}{{2.3.6}{60}{}{theorem.2.3.6}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Convolutional Autoencoder\relax }}{61}{algorithm.5}\protected@file@percent }
\newlabel{alg:convolutional_AE}{{5}{61}{Convolutional Autoencoder\relax }{algorithm.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces The figure illustrates the activations of all $64$ channels in the latent space of our trained convolutional autoencoder, which was optimized with an AMSGrad optimizer, see Algorithm \ref {alg:convolutional_AE}.\relax }}{62}{figure.caption.18}\protected@file@percent }
\newlabel{fig:convolutional_AE_latent}{{2.16}{62}{The figure illustrates the activations of all $64$ channels in the latent space of our trained convolutional autoencoder, which was optimized with an AMSGrad optimizer, see Algorithm \ref {alg:convolutional_AE}.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces On the left side, the figure illustrates $100$ original digits from the MNIST dataset. On the right side, the figure illustrates the same digits after feeding them through the convolutional autoencoder which was optimized with an AMSGrad optimizer.\relax }}{63}{figure.caption.19}\protected@file@percent }
\newlabel{fig:convolutional_AE_inference}{{2.17}{63}{On the left side, the figure illustrates $100$ original digits from the MNIST dataset. On the right side, the figure illustrates the same digits after feeding them through the convolutional autoencoder which was optimized with an AMSGrad optimizer.\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces The figure illustrates the training progresses of the convolutional autoencoder optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis. On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line every $300$ epochs to point out the trend of the training progress.\relax }}{64}{figure.caption.20}\protected@file@percent }
\newlabel{fig:convolutional_AE_training_progress}{{2.18}{64}{The figure illustrates the training progresses of the convolutional autoencoder optimized with an AMSGrad optimizer with epochs on one axis and corresponding training loss on the other axis. On the left side we see the first $3.500$ epochs and on the right side the following epochs until $10.000$. The blue line represents the loss in each epoch and the orange line every $300$ epochs to point out the trend of the training progress.\relax }{figure.caption.20}{}}
\@setckpt{autoencoders}{
\setcounter{page}{65}
\setcounter{equation}{3}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{3}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{18}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{12}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{9}
\setcounter{float@type}{8}
\setcounter{algorithm}{5}
\setcounter{ALG@line}{8}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{theorem}{7}
}
