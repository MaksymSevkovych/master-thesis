\documentclass[11pt, twoside, a4paper]{book}
\usepackage[english,german]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb, amsthm}
\usepackage{amsfonts}
\usepackage{bbold}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[figure]{hypcap}
\usepackage[super]{nth}
\usepackage{tikz}
\usepackage{siunitx}
\usepackage{tikz-cd}
\usepackage{fontenc}
\usetikzlibrary{positioning,calc,spy}
\usepackage{mathtools}
\usepackage{typearea}
\usepackage[outer=2cm, inner=2cm, top=2cm, bottom=25mm,includehead]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage[font=footnotesize,labelfont=bf]{caption}

\usepackage[clines, headsepline, plainfootsepline,automark]{scrlayer-scrpage}
\usepackage{subfig}


%\clearscrheadfoot
\ofoot[scrplain-außen]{scrheadings-außen}
\ihead[]{\headmark}
\chead[]{}
\ohead[]{}
\ifoot[]{}
\cfoot[\pagemark]{\pagemark}
\ofoot[]{}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\R}{\mathbb{R}}
\DeclareMathOperator{\X}{\mathbb{X}}
\DeclareMathOperator{\N}{\mathbb{N}}
\DeclareMathOperator{\Q}{\mathbb{Q}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\D}{\mathcal{D}}
\DeclareMathOperator{\A}{\mathcal{A}}
\DeclareMathOperator{\B}{\mathcal{B}}
\DeclareMathOperator{\F}{\mathcal{F}}
\DeclareMathOperator{\f}{\varphi}
\DeclareMathOperator{\T}{\Theta}
\DeclareMathOperator{\g}{\gamma}
\DeclareMathOperator{\s}{\sigma}
\DeclareMathOperator{\n}{\nu}
\DeclareMathOperator{\e}{\epsilon}
\DeclareMathOperator{\m}{\mu}
\DeclareMathOperator{\risk}{\mathcal{R}}
\DeclareMathOperator{\loss}{\mathcal{L}}
\DeclareMathOperator{\prob}{\mathbb{P}}
\DeclareMathOperator{\p}{\psi}
\DeclareMathOperator{\MSE}{\loss_\text{MSE}}
\DeclareMathOperator{\BCE}{\loss_\text{BCE}}
\DeclareMathOperator{\pdmat}{\mathcal{S}^d_{+}}
\DeclareMathOperator{\diag}{\text{diag}}
\DeclarePairedDelimiterX{\kldiv}[2]{D_{\text{KL}}(}{)}{%
  #1\,\delimsize\|\,#2%
}
\renewcommand{\det}{\text{det}}
\renewcommand{\H}{\mathcal{H}}
\renewcommand{\P}{\Psi}
\renewcommand{\S}{\Sigma}
\renewcommand{\O}{\Omega}
\renewcommand{\t}{\theta}
\renewcommand{\a}{\alpha}
\renewcommand{\b}{\beta}
\renewcommand{\d}{\delta}
\renewcommand{\l}{\lambda}
\renewcommand{\o}{\omega}
\renewcommand{\hat}{\widehat}
\renewcommand{\tilde}{\widetilde}
\newcommand{\id}{\mathrm{id}}
\newcommand*\wc{{}\cdot{}}
\newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{example}[theorem]{Example}

\theoremstyle{plain}
\newcommand{\thistheoremname}{}
\newtheorem{genericthm}[theorem]{\thistheoremname}
\newenvironment{namedthm}[1]
  {\renewcommand{\thistheoremname}{#1}%
   \begin{genericthm}}
  {\end{genericthm}}

\newenvironment{mydescription}[1]
  {\begin{list}{}%
   {\renewcommand\makelabel[1]{##1:\hfill}%
   \settowidth\labelwidth{\makelabel{#1}}%
   \setlength\leftmargin{\labelwidth}
   \addtolength\leftmargin{\labelsep}}}
  {\end{list}}


\title{On Variational Autoencoders: Theory and Applications}
\date{November 22, 2023}
\author{Maksym Sevkovych}

\begin{document}
\selectlanguage{english}
\begin{titlepage}
\vspace*{1 cm}
\begin{center}
\LARGE{Master's thesis}

\vspace{0.5 cm}

\large{November 22, 2023}

\vspace{0.5 cm}

\Huge{\textbf{On Variational Autoencoders: Theory and Applications}}

\vspace{0.5 cm}

\Large{Maksym Sevkovych}

\Large{Registration number: 3330007}

\Large{In collaboration with: DevDuck GmbH}
\vspace{1 cm}

\Large{Inspector: Univ.-Prof. Dr. Ingo Steinwart}
\vspace{3cm}
\end{center}
Recently in the realm of machine learning, the power of generative models has revolutionized the way we perceive data representation and creation. This thesis focuses on the captivating domain of Variational Autoencoders, a cutting-edge class of machine learning models that seamlessly combine unsupervised learning and data generation. In the course of this thesis we embark on an expedition through the intricate architecture and mathematical elegance that underlie Variational Autoencoders.

By dissecting the architecture of Variational Autoencoders, we show their role as both proficient data compressors and imaginative creators. As we navigate the landscapes of latent spaces and probabilistic encodings, we uncover the essential mechanisms driving their flexibility.
Practical applications of Variational Autoencoders extend from anomaly detection to image generation. However, we will focus on the latter in the course of this thesis.
\end{titlepage}
\newpage
\tableofcontents
\newpage
\ihead[]{Introduction}
\section*{Introduction}
In this thesis, our main goal is to understand Variational Autoencoders from a mathematical perspective. Since Variational Autoencoders bring together Bayesian statistics and Deep Learning, we embark on our journey by exploring the basics - covering probability theory, statistics and statistical learning theory, as well as neural networks along with how to optimize them.

After considering the fundamental basics, we continue our expedition by introducing regular Autoencoders. To get a solid grasp, we create various implementations of Autoencoders in different training approaches, which we introduced in the preliminary chapter. We consider all implementations on the MNIST dataset, which contains images of handwritten digits. This hands-on approach helps us build a strong foundation before moving on to Variational Autoencoders.

The shift from Autoencoders to Variational Autoencoders is a crucial part of our exploration. We take a close look at what that sets them apart and consider the theory of Variational Autoencoders in depth, subsequently. Afterwards, we explore practical applications. Through implementing and training multiple Variational Autoencoders, we stumble upon some interesting findings. One notable discovery is that we optimize the common approach of training Variational Autoencoders under certain conditions.

So, in a nutshell, this thesis aims to be a self-contained guide to Variational Autoencoders in a mathematical way, where it combines Bayesian statistics with Deep Learning. Along the way, we share insights that could tweak the usual practices in training Variational Autoencoders, making our exploration both enlightening and practically relevant.


\addcontentsline{toc}{chapter}{Introduction}
\include{preliminary}
\include{autoencoders}
\include{vae}

\addcontentsline{toc}{chapter}{References}
\bibliographystyle{siam}
\bibliography{bibliography}
\newpage
\selectlanguage{german}

\section*{Deutsche Zusammenfassung}
\ihead[]{Deutsche Zusammenfassung}
\addcontentsline{toc}{chapter}{Deutsche Zusammenfassung}

Diese Thesis mit dem Titel \glqq Variational Autoencoders: Theory and Applications\grqq{} behandelt die Kombination aus Bayesianischer Statistik und der Theorie von Autoencodern, einer bestimmten Architektur neuronaler Netze. Die Theorie der neuronalen Netze ist in der Literatur auch bekannt als \glqq Deep Learning\grqq{}. Das Ziel dieser Thesis ist ein grundlegendes mathematisches Verständnis der Variational Autoencoder zu erlangen. Um dies zu erzielen ist die Thesis in drei Teile aufgeteilt. Im ersten Teil werden essentielle Grundlagen aus den Gebieten der Maß- und Wahrschienlichkeitstheorie, der Statistik und Statistischen Lerntheorie, der neuronalen Netze und deren Optimierung behandelt. Im zweiten Teil werden Autoencoder eingeführt und an praktischen Beispielen ausgeführt. Im dritten und letzten Teil werden schließlich VAEs eingeführt und an unterschiedlichen praktischen Beispielen analysiert und veranschaulicht.

Im ersten Kapitel werden dem Lesenden die wichtigsten Grundlagen ausführlich erläutert. Dabei zitieren wir weitesgehend aus renomierter Literatur unterschiedliche Resultate. Zunächst führen wir aus der Maß- und Wahrscheinlichkeitstheorie wichtige Größen wie unter anderem Wahrscheinlichkeitsmaße und -dichten, sowie Zuffallsvariablen ein.
Im Anschluss führen wir die Statistische Lerntheorie ein. Wir erläutern, wie man aus vorhandenen Daten sinnvolle Schlüsse ableiten kann. Genauer gesagt erläutern wir, wie wir eine Funktion finden können, die unsere Daten möglichst gut beschreibt. Unter anderem erläutern wir dabei Größen wie beispielsweise Verlustfunktionen und (empirisches) Risiko.
Als nächstes führen wir neuronale Netze ein und verbinden diese mit der Statistischen Lerntheorie. Dabei erläutern wir ausführlich, wie unterschiedliche Traininsalgorithmen funktionieren. Wir betrachten im Detail den \glqq Gradient Descent\grqq{} Algorithmus und formulieren dazu wichtige Resultate wie beispielsweise den \glqq Backpropagation\grqq{} Algorithmus - eine Methode um Gradienten von hochgradig verketteten Funktionen zu berechnen. Diese Methode ist allgegenwärtig in der Praxis.
Mithilfe des Backpropagation Algorithmus stellen wir fest, wie rechnerisch aufwändig das Training neuronaler Netze sein kann und führen deshalb einen weiteren Optimierungsalgorithmus ein. Dieser neue Algorithmus heißt \glqq Stochastic Gradient Descent\grqq{}. Er funktioniert ähnlich wie der Gradient Descent Algorithmus, ist allerdings deutlich effizienter, da er im Gegensatz zum Gradient Descent Algorithmus nur einen Bruchteil der Daten in jeder Iteration berücksichtigt. Somit benötigt dieser Algorithmus auch nur einen Bruchteil des Rechenaufwands.
Im Zuge der Diskussion über Effizienz führen wir einen weiteren Optimierungsalgorithmus ein, der meist als aktuell optimal angesehen wird. Wir reden dabei über den \glqq Adaptive Moment Estimation\grqq{} (Adam) Algorithmus. Allerdings wurde $2019$ bewiesen, dass dieser Optimierungsalgorithmus in manchen Szenarien doch nicht optimal ist. Stattdessen wurde ein neuer Optimierungsalgorithmus angeregt, welcher \glqq AMSGrad\grqq{} bezeichnet wird. Diesen führen wir ebenfalls ein und analysieren diesen ausführlich. In den darauffolgenden Kapiteln stellen wir ebenfalls anhand von Anwendungsbeispielen fest, dass der AMSGrad Algorithmus bessere Ergebnisse liefert als der Adam Algorithmus.
Als letzte Überlegung im ersten Teil der Thesis führen wir ein, wie neuronale Netze auf Bildern operieren können. Vorangehend führten wir neuronale Netze ein, welche nur auf Vektoren operieren. Durch geschickte Wahl der Operatoren in einem neuronalen Netz ermöglicht es uns, die Struktur von Bildern aufrecht zu erhalten. Dies ist von großem Vorteil, da wir in den darauffolgenden Anwendungen neuronalen Netze auf Bildern operieren lassen wollen.

Im zweiten Kapitel der Thesis betrachten wir Autoencoder. Dabei handelt es sich um eine spezielle Architektur neuronaler Netze, die Daten zunächst in ihrer Dimension reduzieren und im Anschluss wieder auf die ursrpüngliche Dimension zurückführen. Diese Reduktion der Dimensionen wird im Englischen von Softwareentwicklern meistens als \glqq Feature Extraction\grqq{} und von Theoretikern als \glqq Dimensionality Reduction\grqq{} bezeichnet. Sie bietet einige interessante Vorteile und Anwendungsmöglichkeiten. Beispielsweise ist es dadurch möglich Bilder erst zu verkleinern, also deren Auflösung zu reduzieren, und im Anschluss abzuspeichern. Dabei ist eine Verkleinerung von mehreren Größenordnungen möglich. Dadurch lassen sich dementsptechend auf demselben Speicherplatz um Größenordnungen mehr Bilder abspeichern. eine weitere interessante Anwendung ist die Tatsache, dass Machine Learning Modelle genauso auf den \glqq verkleinerten\grqq{} Daten, wie auch auf den Originalen, arbeiten können. Sind die Daten nun aber um Größenordnungern kleiner, so ermöglicht es dem Machine Learning Modell um Größenordnungen mehr Daten in nahezu derselben Zeit zu verarbeiten. Die meisten Modelle in der Industrie derzeit sind so groß, dass sie Tage und manchmal sogar Wochen brauchen, um trainiert zu werden. Beispielsweise wurde Meta's Segment Anything Model (SAM) \glqq 3-5 Tage auf 256 A100 GPUs\grqq trainiert, siehe https://segment-anything.com. Dies verdeutlicht die Relevanz solcher Optimierungen durcdh Dimensionsreduktion. Die letzte Anwendungsmöglichkeit, die wir an dieser Stelle bennen wollen, ist die Möglichkeit von semantischer Suche in verschiedenen Medien. Damit is folgendes gemeint. Beispielsweise trainiert man einen Autoencoder, der Bilder als niedrig-dimensionale Vektoren abspeichern, und wieder reproduzieren kann. Als nächstes trainiert man einen weiteren Autoencoder, der in der Lage ist dasselbe mit Audio-Dateien zu verwirklichen. Nun ist es möglich diese beiden Autoencoder gemeinsam zu trainieren, sodass die niedrig-dimensionalen Vektoren des einen autoencoders mit den Vektoren des zweiten Autoencoders übereinstimmen. Dadurch ermöglicht es uns, dem neuronalen Netz beispielsweise ein Bild von einem Tier bereitzustellen und im Anschluss eine Audio-Datei zu erhalten, welche Geräusche dieses Tier von sich gibt. Beispielsweise gibt man dem neuronalen Netz ein Bild eines Löwen und erhält als Ergebnis das Brüllen eines Löwen als Audio-Datei.

Konkret führen wir im zweiten Kapitel einen wichtigen Ansatz in Anwendungen neuronaler Netze ein. Wir zeigen, dass man neuronale Netze problemlos miteinander verknüpfen kann, sofern die Eingabe- und Ausgabedimension der beiden neuronalen Netze übereinstimmen. Dies lässt sich auf beliebig viele neuronale Netze verallgemeinern. In der Praxis wird dies als \glqq modularer Ansatz\grqq{} beschrieben. Man entwickelt einzelne Module und fügt sie im Nachhinein zusammen.
Als nächstes erläutern wir, inwiefern sich das Training von Autoencodern zum Training allgemeiner neuronaler Netze unterscheidet. Da man bei Autoencodern die Eingabe mit der Ausgabe vergleicht, spricht man hierbei von unüberwachtem Lernen. Im Englischen ist dies als \glqq unsupervised learning\grqq{} bekannt. Zuletzt betrachten wir einige Autoencoder, die wir selber implementiert und trainiert haben. Wir betrachten zu Beginn lineare Autoencoder. Das heißt, dass die Operatoren innerhalb des neuronalen Netzes alle linear und dementsprechen Matrixmultiplikationen sind. Wir evaluieren die Autoencoder auf unterschiedlichste Art und Weise. Zum Beispiel visualisieren wir die niedrig-dimensionalen Darstellungen der Daten, die Rekonstruktionsfehler, den die Autoencoder machen und zeigen zudem, wie die rekonstruierten Bilder aussehen. Des Weiteren betrachten wir den Trainingsverlauf der neuronalen Netze.

Im dritten und damit letzten Kapitel der Thesis betrachten wir Variational Autoencoder. Wir beginnen damit, dass wir den grundlegenden Unterschied zu Autoencodern erläutern. Autoencoder bilden dabei einen Datenpunkt auf einen niedrig-dimensionalen Vektor ab. Im Gegensatz dazu bilden Variational Autoencoder einen Datenpunkt auf eine Wahrscheinlichkeitsdichte, oder genauer gesagt auf eine Wahrschienlichkeitsverteilung, ab. Dies wird dadurch erreicht, dass die Daten erst auf einen niedrig-dimensionalen Vektor abgebildet werden und dieser im Anschluss als Parameter für eine Wahrscheinlichkeitsverteilung dient. In der Industrie ist dabei üblich Normalverteilungen zu betrachten, da diese Wahl einige Berechnungen im Laufe des Trainings deutlich vereinfacht (nicht für jeden Wahrscheinlichkeitsdichte lassen sich gewisse Integrale explizit berechnen).

Im Zuge dieses Kapitels erläutern wir zunächst wie Wahrscheinlichkeitsverteilungen und Wahrscheinlichkeitsdichten zusammenhängen, was uns im Folgenden erlaubt ausschließlich Wahrscheinlichkeitsdichten zu betrachten. Im Anschluss betrachten wir das Grundlegende Resultat der Bayesianischen Statistik, die sogenannte \glqq Bayesianische Formel\grqq{}. Diese ist grundlegend für die Idee der Variational Autoencoder. Des Weiteren fir führen essentielle Begriffe aus der Bayesianischen Statistik, beziehungsweise Bayesianischen Inferenz ein. Konkret lauten diese Begriffe \glqq Prior\grqq{}, \glqq Evidence\grqq{}, \glqq Likelihood\grqq{} und \glqq Posterior\grqq{}. Mit diesen Begriffen motivieren wir den Bayesianischen Zugang zur Lerntheorie und formulieren einen allgemeinen Lernalgorithmus. Als nächstes beziehen wir diesen allgemeinen Lernalgorithmus auf unseren speziellen Fall der Variational Autoencoder. Um dies zu tun, führen wir die Kullback-Leibler Divergenz ein, welche ein relatives Maß ist, welches den Unterschied zwischen Wahrscheinlichkeitsdichten quantifiziert. Demnach ermöglicht dieses Maß uns Wahrscheinlichkeitsdichten miteinander zu vergleichen und dadurch das Training von Variational Autoencoder zu formulieren. Wir beschreiben welche Probleme der Bayesianische Ansatz mit sich bringt und bieten gängige Lösungsansätze. Zusammengefasst lässt sich der Posterior nicht allgemein berechnen und muss deshalb approximiert werden. Diese Approximation lässt sich durch unterschiedliche Art und Weise ermöglichen. Wir betrachten den allgemeinen Ansatz der Literatur und schlagen einen eigenen Ansatz vor, welchen wir mit praktischen Beispielen analysieren.
Als nächstes ermitteln wir verschiedene relevante Resultate der Variational Autoencoder, wie beispielsweise die \glqq Evidence Lower Bound\grqq{} (ELBO). Dies ist, wie der Name vermuten lässt, eine untere Schranke der Evidence, welche eine Große Rolle im Training der Variational Autoencoder spielt. Sie beschreibt sinngemäß den Grenzwert der Kullback-Leibler Divergenz, falls das Training des Variational Autoencoder erfolgreich ist. Abschließend formulieren wir einen konkreten Ansatz eines Variational Autoencoders, wobei wir konkrete Wahlen für den approximierten Posterior und den eigentlichen Posterior treffen. Dadurch ermöglicht es uns eine explizite Darstellung des Trainingsalsgorithmus zu formulieren.

Abschließend im letzten Kapitel implementieren und trainieren wir einige unterschiedliche Variational Autoencoder. Dabei erzeugen wir unterschiedliche Resultate. Wir implementieren einige Variational Autoencoder und analysieren diese. Im Zuge der Analyse zeigen wir deren Rekonstruktionsfähigkeit und die damit verbundenen Rekonsktruktionsfehler. Zudem vergleichen wir diese mit Architekturen des vorangegangenen Kapitels, der herkömmlichen Autoencoder. Außerdem verdeutlichen wir durch Visualisierungen, wie die erzeugten Wahrscheinlichkeitsdichten unterschiedlicher Variational Autoencoder aussehen.
Des Weiteren betrachten wir den Einfluss einer Stellschraube, die wir Kullback-Leibler Koeffizient bezeichnen. Diese Stellschraube kontrolliert das Verhältnis des Einflusses von Kullback-Leibler Divergenz und Verlustfunktion auf das Training des Variational Autoencoder. Dabei betrachten wir die Verlustfunktion als Rekonstruktions-Verlustfunktion und die Kullback-Leibler Divergenz als Regularisierungs-Verlustfunktion. Je höher wir diesen Koeffizient wählen, desto stärker werden die Datenpunkte auf eine Wahrscheinlichkeitsdichte \glqq gedrückt\grqq{}. Das heißt, desto weniger Flexibilität wird dem neuronalen Netz gelassen, selbst eine geeignete niedrig-dimensionale Darstellung zu finden.
Zuletzt hinterfragen wir den in der Literatur üblichen Ansatz des Trainings eines Variational Autoencoder. Dabei wird als approximativer Posterior üblicherweise eine standard Normalverteilung gewählt. Wir hingegen treffen eine andere Wahl. Wir ermitteln dadurch ein interessantes Resultat, welches belegt, dass der allgemeine Ansatz deutlich verbessert werden kann. Durch unterschiedlichste Visualiserungen zeigen wir, dass unsere individuelle Wahl des approximativen Posteriors das Training eines Variational Autoencoder konvergieren lässt, wohingegen der allgemeine Ansatz divergiert.


\vspace{4 cm}

$\overline{\text{Stuttgart, November 22, 2023} \hspace{1 cm}}$


\end{document}
