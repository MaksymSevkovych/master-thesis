\relax
\providecommand\hyper@newdestlabel[2]{}
\citation{goodfellow2016deep}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Preliminary}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{preliminary}{{1}{3}{Preliminary}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Neural networks}{3}{section.1.1}\protected@file@percent }
\newlabel{def_neuron}{{1.1}{4}{}{equation.1.1.1}{}}
\newlabel{def_layer}{{1.1.4}{4}{}{theorem.1.1.4}{}}
\newlabel{eq_linear_layer}{{1.2}{4}{}{equation.1.1.2}{}}
\citation{goodfellow2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A neural network with input $x\in \R  ^4$ and output $y\in \R  ^2$. The five hidden layers have dimensions $3$, $4$, $5$, $3$ and $7$ respectively. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }}{5}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{img_nn}{{1.1}{5}{A neural network with input $x\in \R ^4$ and output $y\in \R ^2$. The five hidden layers have dimensions $3$, $4$, $5$, $3$ and $7$ respectively. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Training of neural networks}{5}{section.1.2}\protected@file@percent }
\newlabel{def:loss}{{1.2.1}{5}{}{theorem.1.2.1}{}}
\citation{lemarechal2012cauchy}
\citation{kantorovich2016functional}
\newlabel{def:risk}{{1.2.3}{6}{}{theorem.1.2.3}{}}
\newlabel{def:empirical_risk}{{1.2.4}{6}{}{theorem.1.2.4}{}}
\citation{kantorovich2016functional}
\newlabel{eq:gd}{{1.4}{7}{Training of neural networks}{equation.1.2.4}{}}
\newlabel{eq:lfe}{{1.5}{7}{Training of neural networks}{equation.1.2.5}{}}
\newlabel{eq:functional}{{1.6}{8}{Training of neural networks}{equation.1.2.6}{}}
\newlabel{theorem:min_lfe}{{1.2.5}{8}{}{theorem.1.2.5}{}}
\newlabel{eq:F_rewritten}{{1.7}{8}{Training of neural networks}{equation.1.2.7}{}}
\newlabel{eq:1.8}{{1.8}{8}{Training of neural networks}{equation.1.2.8}{}}
\newlabel{cor:gd}{{1.2.6}{9}{}{theorem.1.2.6}{}}
\newlabel{eq:F_dir}{{1.9}{9}{Training of neural networks}{equation.1.2.9}{}}
\citation{kantorovich2016functional}
\newlabel{eq:rewritten}{{1.10}{10}{Training of neural networks}{equation.1.2.10}{}}
\newlabel{eq:lower}{{1.11}{10}{Training of neural networks}{equation.1.2.11}{}}
\newlabel{eq:upper}{{1.12}{10}{Training of neural networks}{equation.1.2.12}{}}
\newlabel{eq:x_prime}{{1.13}{10}{Training of neural networks}{equation.1.2.13}{}}
\newlabel{eq:F_rewritten2}{{1.14}{10}{Training of neural networks}{equation.1.2.14}{}}
\newlabel{ineq:1}{{1.15}{10}{Training of neural networks}{equation.1.2.15}{}}
\citation{kantorovich2016functional}
\newlabel{eq:combined}{{1.16}{11}{Training of neural networks}{equation.1.2.16}{}}
\citation{sra2012optimization}
\citation{saad2009line}
\citation{https://doi.org/10.5281/zenodo.4638695}
\citation{lei2019stochastic}
\newlabel{eq:sgd}{{1.17}{13}{Training of neural networks}{equation.1.2.17}{}}
\newlabel{lemma:nn_gradient}{{1.2.8}{13}{}{theorem.1.2.8}{}}
\citation{edition2002probability}
\newlabel{theorem:sgd}{{1.2.10}{14}{}{theorem.1.2.10}{}}
\citation{kingma2014adam}
\citation{goodfellow2016deep}
\citation{reddi2019convergence}
\newlabel{def:moment}{{1.2.11}{15}{}{theorem.1.2.11}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces ADAM optimizer\relax }}{15}{algorithm.1}\protected@file@percent }
\newlabel{alg:adam}{{1}{15}{ADAM optimizer\relax }{algorithm.1}{}}
\newlabel{def:projection}{{1.2.14}{16}{}{theorem.1.2.14}{}}
\newlabel{lemma:projection}{{1.2.15}{16}{}{theorem.1.2.15}{}}
\newlabel{def:regret}{{1.2.16}{16}{}{theorem.1.2.16}{}}
\citation{reddi2019convergence}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Generic Adaptive Method Setup\relax }}{17}{algorithm.2}\protected@file@percent }
\newlabel{alg:general}{{2}{17}{Generic Adaptive Method Setup\relax }{algorithm.2}{}}
\citation{reddi2019convergence}
\citation{reddi2019convergence}
\citation{reddi2019convergence}
\citation{mcmahan2010adaptive}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces AMSGrad Optimizer\relax }}{18}{algorithm.3}\protected@file@percent }
\newlabel{alg:amsgrad}{{3}{18}{AMSGrad Optimizer\relax }{algorithm.3}{}}
\newlabel{lemma:McMahan_Streeter}{{1.2.18}{18}{}{theorem.1.2.18}{}}
\newlabel{eq:ball_def}{{1.19}{18}{Training of neural networks}{equation.1.2.19}{}}
\newlabel{eq:argmin}{{1.2}{18}{Training of neural networks}{equation.1.2.19}{}}
\newlabel{theorem:amsgrad}{{1.2.19}{19}{}{theorem.1.2.19}{}}
\newlabel{eq:1.9}{{1.21}{20}{Training of neural networks}{equation.1.2.21}{}}
\newlabel{eq:1.10}{{1.22}{21}{Training of neural networks}{equation.1.2.22}{}}
\newlabel{eq:1.11}{{1.23}{23}{Training of neural networks}{equation.1.2.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Neural networks in computer vision}{25}{section.1.3}\protected@file@percent }
\newlabel{def:pixel}{{1.3.1}{25}{}{theorem.1.3.1}{}}
\newlabel{def:pixel_domain}{{1.3.3}{26}{}{theorem.1.3.3}{}}
\newlabel{def:image}{{1.3.4}{26}{}{theorem.1.3.4}{}}
\newlabel{lemma:mat_as_array}{{1.3.7}{27}{}{theorem.1.3.7}{}}
\newlabel{def:img_operator}{{1.3.8}{27}{}{theorem.1.3.8}{}}
\citation{goodfellow2016deep}
\newlabel{def:cv_layer}{{1.3.9}{28}{}{theorem.1.3.9}{}}
\newlabel{def:convolution_op}{{1.3.10}{28}{}{theorem.1.3.10}{}}
\newlabel{def:avg_pooling_op}{{1.3.11}{28}{}{theorem.1.3.11}{}}
\newlabel{def:min_pooling_op}{{1.3.12}{29}{}{theorem.1.3.12}{}}
\newlabel{def:max_pooling_op}{{1.3.13}{29}{}{theorem.1.3.13}{}}
\newlabel{prop:convolutional_layer}{{1.3.14}{29}{}{theorem.1.3.14}{}}
\citation{klenke2013probability}
\citation{klenke2013probability}
\citation{klenke2013probability}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Probability and Statistics}{30}{section.1.4}\protected@file@percent }
\newlabel{def:pdf}{{1.4.1}{30}{}{theorem.1.4.1}{}}
\newlabel{def:joint_pdf}{{1.4.2}{30}{}{theorem.1.4.2}{}}
\newlabel{def:cond_prob}{{1.4.4}{30}{}{theorem.1.4.4}{}}
\newlabel{theorem:bayes_rule}{{1.4.5}{31}{}{theorem.1.4.5}{}}
\newlabel{eq:summation_formula}{{1.28}{31}{Probability and Statistics}{equation.1.4.28}{}}
\newlabel{def:parametric_model}{{1.4.6}{31}{}{theorem.1.4.6}{}}
\newlabel{ex:parametric_model}{{1.4.7}{31}{}{theorem.1.4.7}{}}
\citation{cinelli2021variational}
\citation{cinelli2021variational}
\newlabel{theorem:pdf_gen}{{1.4.8}{32}{}{theorem.1.4.8}{}}
\newlabel{ex:loc-scale_fam}{{1.4.9}{32}{}{theorem.1.4.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Illustration of location-scale families for the Gamma distribution and the Gaussian distribution, respectively. In the parametrization of the the Gamma function with the rate parameter $\beta $, the scale parameter as defined in theorem \ref {theorem:pdf_gen} is actually $\s  = 1/\beta $. Note that as the scale $\s  $ increases the distribution becomes less concentrated around the location parameter $\m  $. In particular, $\lim _{\s  \to \infty } p(x; \m  ,\s  ) = \delta (x-\m  )$. \textbf  {(a)} Members of the same scale family of Gamma distributions with shape parameter $\alpha = 2.2$. \textbf  {(b)} Members of the same location-scale family of Gaussian distributions.\relax }}{33}{figure.caption.3}\protected@file@percent }
\newlabel{fig:pdf_gen}{{1.2}{33}{Illustration of location-scale families for the Gamma distribution and the Gaussian distribution, respectively. In the parametrization of the the Gamma function with the rate parameter $\b $, the scale parameter as defined in theorem \ref {theorem:pdf_gen} is actually $\s = 1/\b $. Note that as the scale $\s $ increases the distribution becomes less concentrated around the location parameter $\m $. In particular, $\lim _{\s \to \infty } p(x; \m ,\s ) = \d (x-\m )$. \textbf {(a)} Members of the same scale family of Gamma distributions with shape parameter $\a = 2.2$. \textbf {(b)} Members of the same location-scale family of Gaussian distributions.\relax }{figure.caption.3}{}}
\@setckpt{preliminary}{
\setcounter{page}{34}
\setcounter{equation}{28}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{2}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{8}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{3}
\setcounter{ALG@line}{10}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{theorem}{9}
}
