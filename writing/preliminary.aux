\relax
\providecommand\hyper@newdestlabel[2]{}
\citation{meintrup2006stochastik}
\citation{meintrup2006stochastik}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Preliminaries}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:preliminary}{{1}{5}{Preliminaries}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Probability and Statistics}{5}{section.1.1}\protected@file@percent }
\newlabel{sec:preliminary_prob}{{1.1}{5}{Probability and Statistics}{section.1.1}{}}
\newlabel{def:density}{{1.1.1}{5}{}{theorem.1.1.1}{}}
\newlabel{def:abs_cont}{{1.1.2}{5}{}{theorem.1.1.2}{}}
\citation{meintrup2006stochastik}
\citation{meintrup2006stochastik}
\citation{klenke2013probability}
\citation{klenke2013probability}
\newlabel{theoreom:radon-nikodym}{{1.1.3}{6}{}{theorem.1.1.3}{}}
\newlabel{def:prob_measure}{{1.1.4}{6}{}{theorem.1.1.4}{}}
\newlabel{def:prob_distr_1}{{1.1.5}{6}{}{theorem.1.1.5}{}}
\newlabel{def:prob_distr_2}{{1.1.6}{6}{}{theorem.1.1.6}{}}
\newlabel{def:rv}{{1.1.7}{6}{}{theorem.1.1.7}{}}
\citation{steinwart2008support}
\citation{goodfellow2016deep}
\newlabel{def:distribution}{{1.1.8}{7}{}{theorem.1.1.8}{}}
\newlabel{def:loss}{{1.1.9}{7}{}{theorem.1.1.9}{}}
\newlabel{ex:supervised_loss}{{1.1.10}{7}{}{theorem.1.1.10}{}}
\citation{mucke2019empirical}
\citation{goodfellow2016deep}
\newlabel{def:risk}{{1.1.11}{8}{}{theorem.1.1.11}{}}
\newlabel{def:empirical_risk}{{1.1.12}{8}{}{theorem.1.1.12}{}}
\newlabel{lemma:convex_risk}{{1.1.13}{8}{}{theorem.1.1.13}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Neural Networks}{8}{section.1.2}\protected@file@percent }
\newlabel{sec:nn}{{1.2}{8}{Neural Networks}{section.1.2}{}}
\newlabel{def_neuron}{{1.1}{9}{}{equation.1.2.1}{}}
\newlabel{def:layer}{{1.2.4}{9}{}{theorem.1.2.4}{}}
\newlabel{eq_linear_layer}{{1.2}{9}{}{equation.1.2.2}{}}
\newlabel{def:nn}{{1.2.5}{9}{}{theorem.1.2.5}{}}
\citation{lemarechal2012cauchy}
\citation{kantorovich2016functional}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces A neural network with input $x\in \R  ^4$ and output $y\in \R  ^2$. The five hidden layers have dimensions $3$, $4$, $5$, $3$ and $7$ respectively. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }}{10}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{img_nn}{{1.1}{10}{A neural network with input $x\in \R ^4$ and output $y\in \R ^2$. The five hidden layers have dimensions $3$, $4$, $5$, $3$ and $7$ respectively. The graphic was generated with http://alexlenail.me/NN-SVG/index.html\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Training of Neural Networks}{10}{section.1.3}\protected@file@percent }
\newlabel{sec:training_of_nn}{{1.3}{10}{Training of Neural Networks}{section.1.3}{}}
\citation{kantorovich2016functional}
\newlabel{eq:gd}{{1.4}{11}{Training of Neural Networks}{equation.1.3.4}{}}
\newlabel{eq:lfe}{{1.5}{12}{Training of Neural Networks}{equation.1.3.5}{}}
\newlabel{eq:functional}{{1.6}{12}{Training of Neural Networks}{equation.1.3.6}{}}
\newlabel{theorem:min_lfe}{{1.3.1}{12}{}{theorem.1.3.1}{}}
\newlabel{eq:F_rewritten}{{1.7}{12}{Training of Neural Networks}{equation.1.3.7}{}}
\newlabel{eq:1.8}{{1.8}{12}{Training of Neural Networks}{equation.1.3.8}{}}
\newlabel{cor:gd}{{1.3.2}{13}{}{theorem.1.3.2}{}}
\newlabel{eq:F_dir}{{1.9}{13}{Training of Neural Networks}{equation.1.3.9}{}}
\newlabel{eq:deriv_cond}{{1.10}{13}{Training of Neural Networks}{equation.1.3.10}{}}
\citation{kantorovich2016functional}
\newlabel{theorem:gd}{{1.3.3}{14}{}{theorem.1.3.3}{}}
\newlabel{eq:rewritten}{{1.11}{14}{Training of Neural Networks}{equation.1.3.11}{}}
\newlabel{eq:lower}{{1.12}{14}{Training of Neural Networks}{equation.1.3.12}{}}
\newlabel{eq:upper}{{1.13}{14}{Training of Neural Networks}{equation.1.3.13}{}}
\newlabel{eq:x_prime}{{1.14}{15}{Training of Neural Networks}{equation.1.3.14}{}}
\newlabel{eq:F_rewritten2}{{1.15}{15}{Training of Neural Networks}{equation.1.3.15}{}}
\newlabel{ineq:1}{{1.16}{15}{Training of Neural Networks}{equation.1.3.16}{}}
\newlabel{eq:1.16}{{1.17}{15}{Training of Neural Networks}{equation.1.3.17}{}}
\citation{kantorovich2016functional}
\newlabel{eq:combined}{{1.18}{16}{Training of Neural Networks}{equation.1.3.18}{}}
\newlabel{ex:nn_gradients_1dim}{{1.3.4}{16}{}{theorem.1.3.4}{}}
\citation{simmons1995calculus}
\newlabel{eq:nn1}{{1.19}{17}{}{equation.1.3.19}{}}
\newlabel{eq:risk_expl}{{1.20}{17}{}{equation.1.3.20}{}}
\newlabel{eq:chain_rule}{{1.21}{17}{}{equation.1.3.21}{}}
\newlabel{eq:loss_expl}{{1.22}{17}{}{equation.1.3.22}{}}
\newlabel{eq:grad_split}{{1.23}{18}{}{equation.1.3.23}{}}
\citation{goodfellow2016deep}
\newlabel{theorem:multi_var_chainrule}{{1.3.5}{20}{}{theorem.1.3.5}{}}
\newlabel{eq:nn_loss}{{1.24}{21}{Training of Neural Networks}{equation.1.3.24}{}}
\newlabel{eq:part1}{{1.25}{21}{Training of Neural Networks}{equation.1.3.25}{}}
\newlabel{eq:part2}{{1.26}{21}{Training of Neural Networks}{equation.1.3.26}{}}
\newlabel{eq:part3}{{1.27}{22}{Training of Neural Networks}{equation.1.3.27}{}}
\citation{sra2012optimization}
\citation{saad2009line}
\citation{turinici2021convergence}
\citation{lei2019stochastic}
\citation{klenke2013probability}
\newlabel{eq:sgd}{{1.28}{24}{Training of Neural Networks}{equation.1.3.28}{}}
\newlabel{ass:sgd}{{1.3.6}{24}{}{theorem.1.3.6}{}}
\newlabel{eq:ass1}{{1.29}{24}{}{equation.1.3.29}{}}
\newlabel{eq:ass2}{{1.30}{24}{}{equation.1.3.30}{}}
\newlabel{theorem:sgd}{{1.3.7}{24}{}{theorem.1.3.7}{}}
\newlabel{ass1}{{1}{24}{}{Item.5}{}}
\newlabel{ass2}{{2}{24}{}{Item.6}{}}
\newlabel{eq:dist}{{1.31}{24}{}{equation.1.3.31}{}}
\newlabel{eq:dist_rec}{{1.32}{24}{}{equation.1.3.32}{}}
\newlabel{ass3}{{3}{24}{}{Item.7}{}}
\newlabel{eq:limsup}{{1.33}{24}{}{equation.1.3.33}{}}
\newlabel{ass4}{{4}{25}{}{Item.8}{}}
\newlabel{eq:step_size}{{1.34}{25}{}{equation.1.3.34}{}}
\newlabel{eq:item2}{{1.35}{25}{Training of Neural Networks}{equation.1.3.35}{}}
\newlabel{ineq:2}{{1.36}{25}{Training of Neural Networks}{equation.1.3.36}{}}
\citation{papoulis02}
\newlabel{ineq:3}{{1.37}{26}{Training of Neural Networks}{equation.1.3.37}{}}
\newlabel{ineq:iter}{{1.38}{26}{Training of Neural Networks}{equation.1.3.38}{}}
\citation{meintrup2006stochastik}
\citation{kingma2014adam}
\citation{goodfellow2016deep}
\newlabel{def:moment}{{1.3.8}{27}{}{theorem.1.3.8}{}}
\newlabel{def:moment_gen}{{1.3.9}{27}{}{theorem.1.3.9}{}}
\citation{reddi2019convergence}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Adam optimizer\relax }}{28}{algorithm.1}\protected@file@percent }
\newlabel{alg:adam}{{1}{28}{Adam optimizer\relax }{algorithm.1}{}}
\newlabel{def:projection}{{1.3.12}{28}{}{theorem.1.3.12}{}}
\newlabel{lemma:projection}{{1.3.13}{28}{}{theorem.1.3.13}{}}
\newlabel{def:regret}{{1.3.14}{29}{}{theorem.1.3.14}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Generic Adaptive Method Setup\relax }}{29}{algorithm.2}\protected@file@percent }
\newlabel{alg:general}{{2}{29}{Generic Adaptive Method Setup\relax }{algorithm.2}{}}
\citation{reddi2019convergence}
\citation{reddi2019convergence}
\citation{reddi2019convergence}
\citation{reddi2019convergence}
\citation{mcmahan2010adaptive}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces AMSGrad Optimizer\relax }}{30}{algorithm.3}\protected@file@percent }
\newlabel{alg:amsgrad}{{3}{30}{AMSGrad Optimizer\relax }{algorithm.3}{}}
\newlabel{lemma:McMahan_Streeter}{{1.3.16}{30}{}{theorem.1.3.16}{}}
\newlabel{eq:ball_def}{{1.39}{31}{Training of Neural Networks}{equation.1.3.39}{}}
\newlabel{eq:argmin}{{1.40}{31}{Training of Neural Networks}{equation.1.3.40}{}}
\newlabel{eq:grad}{{1.41}{31}{Training of Neural Networks}{equation.1.3.41}{}}
\newlabel{ineq:convex}{{1.42}{31}{Training of Neural Networks}{equation.1.3.42}{}}
\newlabel{theorem:amsgrad}{{1.3.17}{32}{}{theorem.1.3.17}{}}
\citation{jahn2009vector}
\newlabel{eq:1.9}{{1.43}{33}{Training of Neural Networks}{equation.1.3.43}{}}
\newlabel{eq:1.10}{{1.44}{34}{Training of Neural Networks}{equation.1.3.44}{}}
\newlabel{eq:1.11}{{1.45}{35}{Training of Neural Networks}{equation.1.3.45}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Neural Networks in Computer Vision}{38}{section.1.4}\protected@file@percent }
\newlabel{def:pixel}{{1.4.1}{38}{}{theorem.1.4.1}{}}
\newlabel{def:pixel_domain}{{1.4.3}{38}{}{theorem.1.4.3}{}}
\newlabel{def:image}{{1.4.4}{38}{}{theorem.1.4.4}{}}
\newlabel{lemma:mat_as_array}{{1.4.7}{39}{}{theorem.1.4.7}{}}
\citation{goodfellow2016deep}
\newlabel{def:img_operator}{{1.4.8}{40}{}{theorem.1.4.8}{}}
\newlabel{def:cv_layer}{{1.4.9}{40}{}{theorem.1.4.9}{}}
\newlabel{def:convolution_op}{{1.4.10}{40}{}{theorem.1.4.10}{}}
\newlabel{def:transposed_convolution_op}{{1.4.11}{41}{}{theorem.1.4.11}{}}
\newlabel{def:avg_pooling_op}{{1.4.12}{41}{}{theorem.1.4.12}{}}
\newlabel{def:min_pooling_op}{{1.4.13}{42}{}{theorem.1.4.13}{}}
\newlabel{def:max_pooling_op}{{1.4.14}{42}{}{theorem.1.4.14}{}}
\newlabel{def:convolutional_layer}{{1.4.15}{42}{}{theorem.1.4.15}{}}
\@setckpt{preliminary}{
\setcounter{page}{43}
\setcounter{equation}{50}
\setcounter{enumi}{2}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{1}
\setcounter{table}{0}
\setcounter{parentequation}{0}
\setcounter{section@level}{1}
\setcounter{Item}{10}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{5}
\setcounter{float@type}{8}
\setcounter{algorithm}{3}
\setcounter{ALG@line}{10}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{KVtest}{0}
\setcounter{subfigure}{0}
\setcounter{subfigure@save}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{subtable@save}{0}
\setcounter{lotdepth}{1}
\setcounter{theorem}{15}
}
