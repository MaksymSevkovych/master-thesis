\chapter{Implementation}

In this final chapter we want to consider a possible way to implement the theory. We postulate an algorithm how to apply the theory and afterwards, we construct a specific example of how to implement and visualize the theory.

\section{Algorithm}

In order to implement the theory, we have to consider the Tikhonov regularization method

\begin{align*}
\T_{\a;y_\d} = \min_{x \in D} \left( \D(\F(x), y_\d) + \a \reg(x) \right).
\end{align*}

Hence, we have to choose a certain similarity measure $\D: Y \times Y \to \R$, e.g.: $\D(y_1,y_2) = \frac{1}{2}\|y_1-y_2\|^2$ the mean squared error. Additionally, we have to define the regularizer. This we do by proposing for $q\geq 1$

\begin{align*}
\reg(\V,\wc) = \sum_{\l\in\L} \left\|\Phi_{\l}(\V,x) \right\|_{q}^q,
\end{align*}

with a neural network $\Phi(\V,\wc) = (\Phi(\V_\l,\wc))_{\l\in\L}$ as in equation \eqref{Def_Phi}. This ultimately resembles a non-linear $\ell^q$-regularizer. Combining the above two definitions, this leads to the optimization problem

\begin{align}\label{optim}
\T_{\a;y}(x) = \min_{x \in D} \left\{\frac{1}{2}\left\|\F(x) - y_\d\right\|^2 + \a \sum_{\l\in\L_l} \left\|\Phi_{\l}(\V,x) \right\|_{q}^q \right\}.
\end{align}

At this point, we should note that $\Phi(\V,\wc)$ is an already pre-trained network. Therefore, the optimization problem \eqref{optim} can be tackled by using an incremental gradient descent method, since it is easy for standard software (e.g.: pyTorch, TensorFlow, etc.) to compute the gradient of the regularizer $\reg(\V,\wc)$. The gradient of the first term $\frac{1}{2}|\F(x) - y_\d\|^2$ can be easily computed with standard functional analytic methods as well, see \cite{werner2006funktionalanalysis}.
\newpage
All in all, we propose the algorithm \ref{alg1} for an incremental gradient descent iteration to minimize the network Tikhonov method.

\begin{algorithm}
\caption{Incremental gradient descent for NETT}\label{alg1}
\begin{algorithmic}[1]
\State Choose a family of step-sizes $(\g_k)_{k\in\N} \subset (0,\infty)^\N$
\State Choose an initial guess $x_0 \in D \subseteq X$.
\For{$i = 1$ to maxiter}
    \State $\bar{x}_i \gets x_{i-1} - \g_i\F^\prime(x_{i-1})(\F(x_{i-1}) - y_\d)$ \Comment{gradient descent step for first term}
	\State $x_i \gets \bar{x}_i - \g_i \a \nabla_{x} \reg(\V,\wc)(\bar{x}_i)$ \Comment{gradient descent step for second term}
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Specific example}

This section is not based on the paper \cite{li2020nett} by Housen Li, Johannes Schwab, Stephan Antholzer and Markus Haltmeier any more, these are new ideas.\\
Engaging Machine Learning tasks one may face to crucial decisions - what data should I use to train? And the second one being what model represents by data best? Since there are many pre-trained models out there, the idea is to choose a pre-trained neural network and choose as well the dataset, the model has been trained on. This way we can test the approximation at all times. Since our main goal was

\begin{align}
\text{Estimate } x \in D \text{ from data } y_\d = \F(x) + \x_\d.
\end{align}

Since $y_\d$ is the observed data and $\x_\d$ is additive noise, we thereby artificially construct a new dataset by applying $\F(x) + \x_\d$ on each data point of the original data set. We may choose a transformation $\F(\wc)$ as we see fit, for example by adding any kind of noise or filter. Since we add the altercation ourselves, we can lead each altered data point back to its original value. This gives us the opportunity to compare the original value to the value the NETT method yields. Hereby, we tackled the first major question, what data to use.\\
The second question - what model to use - is tackled fairly quickly as described above, since we already chose a pre-trained model on the original dataset. We simply have to choose it to be a neural network, in order to stay in the network Tikhonov setting. Otherwise, we could use any other arbitrary pre-trained model that satisfies our assumptions \ref{ass2}. This would lead to the general Tikhonov method.\\
There are many communities providing already pre-trained models as described above. An example for such a community would be huggingface.co. It offers a huge selection of state of the art models and corresponding datasets for computer vision, NLP, audio recognition, and many other tasks.
